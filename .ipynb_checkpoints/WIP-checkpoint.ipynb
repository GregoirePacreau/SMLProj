{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polarity prediction on movie reviews, comparison of parameters and learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "(scikit-learn for learning, nltk for text processing and pandas for data reprensentations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning methods\n",
    "To classify a vector of numbers, we used :\n",
    "\n",
    "- Logistic regression\n",
    "- MultinomialNB\n",
    "- kNN\n",
    "- Random Forest\n",
    "\n",
    "## Text representations\n",
    "We tried different representations of data to see how it influences the learning :\n",
    "- Bag-of-words\n",
    "- n-grams\n",
    "- Term frequency (bag-of-words normalized)\n",
    "- Term frequency times inverse document frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional filter on the training set\n",
    "\n",
    "We will try to remove English stop words: this includes 1-grams and 2-grams (we did not take into account 3-grams because of the length of the vector).\n",
    "\n",
    "We ignore terms that appear in more than 70% of the documents, which is intuitively meaningful: with 60% of word frequency in english, we still find words such as \"uh\" in our texts. We also only keep terms that appear in at least 2 documents not to generalize on a very specific example.\n",
    "\n",
    "We did keep in mind that removing \"stop-words\" can lead in information loss. The tfidf representation already decreases the influence of stop-words, but we wanted to get rid of the more common ones to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "from scikitplot.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the datasets\n",
    "(We used the small dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>neg</td>\n",
       "      <td>this talky , terribly-plotted thriller stars a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>pos</td>\n",
       "      <td>the sweet hereafter could serve as a textbook ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>neg</td>\n",
       "      <td>at one point in this movie there is a staging ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>pos</td>\n",
       "      <td>marie ( charlotte rampling , \" aberdeen \" ) an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>neg</td>\n",
       "      <td>an affluent horse breeder's past comes up to h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text  label_num\n",
       "1729   neg  this talky , terribly-plotted thriller stars a...          0\n",
       "484    pos  the sweet hereafter could serve as a textbook ...          1\n",
       "1074   neg  at one point in this movie there is a staging ...          0\n",
       "279    pos  marie ( charlotte rampling , \" aberdeen \" ) an...          1\n",
       "1032   neg  an affluent horse breeder's past comes up to h...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all files path\n",
    "posFiles = glob('review_polarity/txt_sentoken/pos/*')\n",
    "negFiles = glob('review_polarity/txt_sentoken/neg/*')\n",
    "# Read text files\n",
    "posReviews = np.array([open(f).read() for f in posFiles])\n",
    "negReviews = np.array([open(f).read() for f in negFiles])\n",
    "# Use pandas to label, mix the data and print a sample\n",
    "polarity_files_df = pd.DataFrame({'pos':posReviews,'neg':negReviews})\n",
    "polarity_files_df = pd.melt(polarity_files_df, value_vars=['pos','neg'],value_name=\"text\",var_name=\"label\")\n",
    "polarity_files_df[\"label_num\"] = polarity_files_df.label.map({\"neg\":0, \"pos\":1})\n",
    "polarity_files_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger dataset and smaller one are two differents dataset. We thus only compare results acquired on the smaller dataset. However we still give results computed with larger dataset to have an idea of the behavior of a method with an other dataset.\n",
    "(Results on the big dataset have been found by using the same code than the one used for small dataset but only using the train set as below. It allowed us to avoid adapting the code for a larger dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files path\n",
    "#posFiles = glob('aclImdb_v1/aclImdb/test/pos/*')\n",
    "#negFiles = glob('aclImdb_v1/aclImdb/test/neg/*')\n",
    "# Read text files\n",
    "#posReviews = np.array([open(f).read() for f in posFiles])\n",
    "#negReviews = np.array([open(f).read() for f in negFiles])\n",
    "# Use pandas to label, mix the data and print a sample\n",
    "#polarity_files_df = pd.DataFrame({'pos':posReviews,'neg':negReviews})\n",
    "#polarity_files_df = pd.melt(polarity_files_df, value_vars=['pos','neg'],value_name=\"text\",var_name=\"label\")\n",
    "#polarity_files_df[\"label_num\"] = polarity_files_df.label.map({\"neg\":0, \"pos\":1})\n",
    "#polarity_files_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and shuffle the data\n",
    "Used when comparing confusion matrices. We also have a look to the length of the vectors after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700,)\n",
      "(300,)\n",
      "(1700,)\n",
      "(300,)\n",
      "22142\n",
      "36990\n",
      "36993\n",
      "37298\n"
     ]
    }
   ],
   "source": [
    "# Split and shuffle the data (15% for train and 85% for tests)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(polarity_files_df.text, polarity_files_df.label_num, test_size=0.15, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "vec1 = CountVectorizer(stop_words='english', max_df =0.7, min_df=2)\n",
    "vec1.fit_transform(X_train)\n",
    "print(len(vec1.get_feature_names()))\n",
    "\n",
    "\n",
    "vec2 = CountVectorizer(stop_words='english', max_df =0.7)\n",
    "vec2.fit_transform(X_train)\n",
    "print(len(vec2.get_feature_names()))\n",
    "\n",
    "#removing max_df=0.7 changes nothing but keeping it seems meaningful for larger datasets\n",
    "vec3 = CountVectorizer(stop_words='english')\n",
    "vec3.fit_transform(X_train)\n",
    "print(len(vec3.get_feature_names()))\n",
    "\n",
    "\n",
    "vec4 = CountVectorizer()\n",
    "vec4.fit_transform(X_train)\n",
    "print(len(vec4.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        ...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'lr__C': (0.5, 1, 2, 20), 'tfidf__use_idf': (True, False), 'vect__ngram_range': [(1, 1), (1, 2)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test a range of hyperparameters\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', max_df =0.7, min_df=2)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('lr', LogisticRegression())\n",
    "                    ])\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf' : (True,False),\n",
    "              'lr__C' : (0.5,1,2,20)\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr__C: 20\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 1)\n",
      "0.836470588235\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr__C</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>0.836471</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0.834706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>0.833529</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.832941</td>\n",
       "      <td>0.994117</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.825294</td>\n",
       "      <td>0.987059</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0.825294</td>\n",
       "      <td>0.996176</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0.825294</td>\n",
       "      <td>0.979706</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0.825294</td>\n",
       "      <td>0.985589</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.973825</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lr__C  mean_test_score  mean_train_score ngram_range  tfidf\n",
       "rank                                                            \n",
       "1       20         0.836471          1.000000      (1, 1)   True\n",
       "2       20         0.835294          1.000000      (1, 2)   True\n",
       "3       20         0.834706          1.000000      (1, 2)  False\n",
       "4       20         0.833529          1.000000      (1, 1)  False\n",
       "5        2         0.832941          0.994117      (1, 1)   True\n",
       "6        1         0.825294          0.987059      (1, 1)   True\n",
       "6        2         0.825294          0.996176      (1, 2)   True\n",
       "6        2         0.825294          0.979706      (1, 1)  False\n",
       "6        2         0.825294          0.985589      (1, 2)  False\n",
       "10     0.5         0.820000          0.973825      (1, 1)   True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the best scores\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "\n",
    "print(gs_clf.best_score_)\n",
    "\n",
    "df = pd.DataFrame({'rank':gs_clf.cv_results_['rank_test_score'], \n",
    "                  'ngram_range':gs_clf.cv_results_['param_vect__ngram_range'],\n",
    "                  'tfidf':gs_clf.cv_results_['param_tfidf__use_idf'],\n",
    "                  'lr__C':gs_clf.cv_results_['param_lr__C'],\n",
    "                  'mean_test_score':gs_clf.cv_results_['mean_test_score'], \n",
    "                  'mean_train_score':gs_clf.cv_results_['mean_train_score']}).set_index('rank')\n",
    "\n",
    "#Classifier less regularized (large C) can better specialized but generalize as well as the others\n",
    "#as shown below\n",
    "df.sort_values('rank',ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix on bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the larger dataset, we had 0.89 accuracy by using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos       0.82      0.85      0.83       151\n",
      "         neg       0.84      0.81      0.83       149\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       300\n",
      "   macro avg       0.83      0.83      0.83       300\n",
      "weighted avg       0.83      0.83      0.83       300\n",
      "\n",
      "0.83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6c2771b470>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.colors' has no attribute 'to_rgba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     37\u001b[0m             display(\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             )\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36m_fetch_figure_metadata\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;34m\"\"\"Get some metadata to help with displaying a figure.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;31m# determine if a background is needed for legibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_facecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;31m# the background is transparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         ticksLight = _is_light([label.get_color()\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36m_is_transparent\u001b[0;34m(color)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;34m\"\"\"Determine transparency from alpha.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrgba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.colors' has no attribute 'to_rgba'"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range =(1,2), max_df =0.7, min_df=2)),\n",
    "                     ('nb', LogisticRegression()),\n",
    "                    ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[\"pos\",\"neg\"]))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, normalize=True, title='Normalized confusion matrix - Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        ...linear_tf=False, use_idf=True)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'nb__alpha': (0, 1, 2, 10), 'tfidf__use_idf': (True, False), 'vect__ngram_range': [(1, 1), (1, 2)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test a range of hyperparameters\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', max_df =0.7, min_df=2)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('nb', MultinomialNB())\n",
    "                    ])\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf' : (True,False),\n",
    "              'nb__alpha': (0,1,2,10)\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb__alpha: 1\n",
      "tfidf__use_idf: False\n",
      "vect__ngram_range: (1, 2)\n",
      "0.815294117647\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>nb__alpha</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.815294</td>\n",
       "      <td>0.970295</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.814706</td>\n",
       "      <td>0.957648</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.814706</td>\n",
       "      <td>0.959708</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.809412</td>\n",
       "      <td>0.981470</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.809412</td>\n",
       "      <td>0.948827</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.806471</td>\n",
       "      <td>0.984705</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.804706</td>\n",
       "      <td>0.972354</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.802353</td>\n",
       "      <td>0.974119</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.801765</td>\n",
       "      <td>0.968825</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.801765</td>\n",
       "      <td>0.959709</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_test_score  mean_train_score nb__alpha ngram_range  tfidf\n",
       "rank                                                                \n",
       "1            0.815294          0.970295         1      (1, 2)  False\n",
       "2            0.814706          0.957648         1      (1, 1)  False\n",
       "2            0.814706          0.959708         2      (1, 2)  False\n",
       "4            0.809412          0.981470         2      (1, 2)   True\n",
       "4            0.809412          0.948827         2      (1, 1)  False\n",
       "6            0.806471          0.984705         1      (1, 2)   True\n",
       "7            0.804706          0.972354        10      (1, 2)   True\n",
       "8            0.802353          0.974119         1      (1, 1)   True\n",
       "9            0.801765          0.968825         2      (1, 1)   True\n",
       "9            0.801765          0.959709        10      (1, 1)   True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the 10 best scores\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "\n",
    "print(gs_clf.best_score_)\n",
    "\n",
    "df = pd.DataFrame({'rank':gs_clf.cv_results_['rank_test_score'], \n",
    "                  'ngram_range':gs_clf.cv_results_['param_vect__ngram_range'],\n",
    "                  'tfidf':gs_clf.cv_results_['param_tfidf__use_idf'],\n",
    "                  'nb__alpha':gs_clf.cv_results_['param_nb__alpha'],\n",
    "                  'mean_test_score':gs_clf.cv_results_['mean_test_score'], \n",
    "                  'mean_train_score':gs_clf.cv_results_['mean_train_score']}).set_index('rank')\n",
    "\n",
    "df.sort_values('rank',ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix on bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the larger dataset, we had 0.88 accuracy by using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos       0.81      0.77      0.79       151\n",
      "         neg       0.78      0.81      0.80       149\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       300\n",
      "   macro avg       0.79      0.79      0.79       300\n",
      "weighted avg       0.79      0.79      0.79       300\n",
      "\n",
      "0.793333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6c245f97f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.colors' has no attribute 'to_rgba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     37\u001b[0m             display(\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             )\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36m_fetch_figure_metadata\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;34m\"\"\"Get some metadata to help with displaying a figure.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;31m# determine if a background is needed for legibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_facecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;31m# the background is transparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         ticksLight = _is_light([label.get_color()\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36m_is_transparent\u001b[0;34m(color)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;34m\"\"\"Determine transparency from alpha.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrgba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.colors' has no attribute 'to_rgba'"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range =(1,2), max_df =0.7, min_df=2)),\n",
    "                     ('nb', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[\"pos\",\"neg\"]))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, normalize=True, title='Normalized confusion matrix - Multinomial Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Test a range of hyperparameters\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', max_df =0.7, min_df=2)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('knn', KNeighborsClassifier()),\n",
    "                    ])\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf' : (True,False),\n",
    "              'knn__n_neighbors': (10, 25, 50),\n",
    "              'knn__p' : (1,2),\n",
    "              'knn__weights': ('uniform', 'distance')\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the 10 best scores\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "\n",
    "print(gs_clf.best_score_)\n",
    "\n",
    "df = pd.DataFrame({'rank':gs_clf.cv_results_['rank_test_score'], \n",
    "                  'ngram_range':gs_clf.cv_results_['param_vect__ngram_range'],\n",
    "                  'tfidf':gs_clf.cv_results_['param_tfidf__use_idf'],\n",
    "                  'knn__n_neighbors': gs_clf.cv_results_['param_knn__n_neighbors'],\n",
    "                  'knn__p': gs_clf.cv_results_['param_knn__p'], \n",
    "                  'knn__weights': gs_clf.cv_results_['param_knn__weights'], \n",
    "                  'mean_test_score':gs_clf.cv_results_['mean_test_score'], \n",
    "                  'mean_train_score':gs_clf.cv_results_['mean_train_score']}).set_index('rank')\n",
    "\n",
    "df.sort_values('rank',ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix on bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the larger dataset, we had 0.54 accuracy by using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range =(1,2), max_df =0.7, min_df=2)),\n",
    "                     ('nb', KNeighborsClassifier()),\n",
    "                    ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[\"pos\",\"neg\"]))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, normalize=True, title='Normalized confusion matrix - K-Nearest Neighbors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a range of hyperparameters\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', max_df =0.7, min_df=2)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('rdc', RandomForestClassifier())\n",
    "                    ])\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf' : (True,False),\n",
    "              'rdc__n_estimators': (10, 25, 50)\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the 10 best scores\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "\n",
    "print(gs_clf.best_score_)\n",
    "\n",
    "df = pd.DataFrame({'rank':gs_clf.cv_results_['rank_test_score'], \n",
    "                  'ngram_range':gs_clf.cv_results_['param_vect__ngram_range'],\n",
    "                  'tfidf':gs_clf.cv_results_['param_tfidf__use_idf'],\n",
    "                  'rdc__n_estimators': gs_clf.cv_results_['param_rdc__n_estimators'],\n",
    "                  'mean_test_score':gs_clf.cv_results_['mean_test_score'], \n",
    "                  'mean_train_score':gs_clf.cv_results_['mean_train_score']\n",
    "                  }).set_index('rank')\n",
    "\n",
    "df.sort_values('rank',ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix on bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the larger dataset, we had 0.77 accuracy by using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range =(1,2), max_df =0.7, min_df=2)),\n",
    "                     ('rdc', RandomForestClassifier()),\n",
    "                    ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=[\"pos\",\"neg\"]))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, normalize=True, title='Normalized confusion matrix - Random Forest')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
