{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from scikitplot.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>neg</td>\n",
       "      <td>capsule : a ham-handed and over/underwritten m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>pos</td>\n",
       "      <td>\" when you get out of jail , you can kill him...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>pos</td>\n",
       "      <td>natural born killers is really a very simple s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>neg</td>\n",
       "      <td>note : some may consider portions of the follo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>neg</td>\n",
       "      <td>a couple of criminals ( mario van peebles and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text  label_num\n",
       "1420   neg  capsule : a ham-handed and over/underwritten m...          0\n",
       "978    pos   \" when you get out of jail , you can kill him...          1\n",
       "637    pos  natural born killers is really a very simple s...          1\n",
       "1123   neg  note : some may consider portions of the follo...          0\n",
       "1429   neg  a couple of criminals ( mario van peebles and ...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get all files path\n",
    "posFiles = glob('review_polarity/txt_sentoken/pos/*')\n",
    "negFiles = glob('review_polarity/txt_sentoken/neg/*')\n",
    "#read text files\n",
    "posReviews = np.array([open(f).read() for f in posFiles])\n",
    "negReviews = np.array([open(f).read() for f in negFiles])\n",
    "#use pandas to label and mix the data\n",
    "polarity_files_df = pd.DataFrame({'pos':posReviews,'neg':negReviews})\n",
    "polarity_files_df = pd.melt(polarity_files_df, value_vars=['pos','neg'],value_name=\"text\",var_name=\"label\")\n",
    "polarity_files_df[\"label_num\"] = polarity_files_df.label.map({\"neg\":0, \"pos\":1})\n",
    "polarity_files_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340,)\n",
      "(660,)\n",
      "(1340,)\n",
      "(660,)\n"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(polarity_files_df.text, polarity_files_df.label_num, test_size=0.33, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos       0.81      0.80      0.80       335\n",
      "         neg       0.79      0.80      0.80       325\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       660\n",
      "   macro avg       0.80      0.80      0.80       660\n",
      "weighted avg       0.80      0.80      0.80       660\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove English stop words\n",
    "# include 1-grams and 2-grams (if 3-grams : seems not better because of the length of the vector)\n",
    "# ignore terms that appear in more than 70% of the documents (intuitively meaningful, it is indeed the best multiple of 10% to have a good score )\n",
    "# only keep terms that appear in at least 2 documents\n",
    "\n",
    "# TODO : remove 'not' etc. from stopwords to not remove 'not' from sentences like 'this film is not bad'\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range =(1,2), max_df =0.7, min_df=2)),\n",
    "                     ('nb', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "# à vérifier : pos et neg sont biens les pos et neg\n",
    "print(metrics.classification_report(y_test, y_pred,\n",
    "    target_names=[\"pos\",\"neg\"]))\n",
    "metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "#plt.figure()\n",
    "#plot_confusion_matrix(cnf_matrix, classes=['pos','neg'],\n",
    "#                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "#plt.figure()\n",
    "#plot_confusion_matrix(cnf_matrix, classes=['pos','neg'], normalize=True,\n",
    "#                      title='Normalized confusion matrix')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
       "           ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
       "         ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'nb': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 0.7,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 2,\n",
       " 'vect__ngram_range': (1, 3),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': 'english',\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': None,\n",
       " 'vect__vocabulary': None,\n",
       " 'nb__alpha': 1.0,\n",
       " 'nb__class_prior': None,\n",
       " 'nb__fit_prior': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test a range of hyperparameters\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range =(1,3), max_df =0.7, min_df=2)),\n",
    "                     ('nb', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "text_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#TO ADAPT depending on what text_clf.get_params() displays\n",
    "#how did we look for a good parameter alpha : try with 1e-3 and 1e-2 : 1e-2 is the best so we tried with 1e0 and 1e-1 and 1e-2 : 1e-1 is the best one\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'nb__alpha': (1e-2, 1e-1),\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7805970149253731"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "\n",
    "gs_clf.best_score_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb__alpha: 0.1\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('nb', MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_estimator_.get_params()[\"nb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.86149295, 2.63325866, 0.78922749, 2.75457239]),\n",
       " 'std_fit_time': array([0.04743894, 0.08169105, 0.03576745, 0.24032973]),\n",
       " 'mean_score_time': array([0.35999886, 0.56723094, 0.37510864, 0.51553424]),\n",
       " 'std_score_time': array([0.00871117, 0.03332667, 0.02796307, 0.10340577]),\n",
       " 'param_nb__alpha': masked_array(data=[0.01, 0.01, 0.1, 0.1],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 1), (1, 2)],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'nb__alpha': 0.01, 'vect__ngram_range': (1, 1)},\n",
       "  {'nb__alpha': 0.01, 'vect__ngram_range': (1, 2)},\n",
       "  {'nb__alpha': 0.1, 'vect__ngram_range': (1, 1)},\n",
       "  {'nb__alpha': 0.1, 'vect__ngram_range': (1, 2)}],\n",
       " 'split0_test_score': array([0.75615213, 0.74496644, 0.78299776, 0.75838926]),\n",
       " 'split1_test_score': array([0.7360179 , 0.75167785, 0.7606264 , 0.77628635]),\n",
       " 'split2_test_score': array([0.78699552, 0.78475336, 0.79820628, 0.79820628]),\n",
       " 'mean_test_score': array([0.75970149, 0.76044776, 0.78059701, 0.77761194]),\n",
       " 'std_test_score': array([0.02095861, 0.01738485, 0.0154328 , 0.01627922]),\n",
       " 'rank_test_score': array([4, 3, 1, 2], dtype=int32),\n",
       " 'split0_train_score': array([0.99888018, 1.        , 0.99776036, 1.        ]),\n",
       " 'split1_train_score': array([0.99664054, 1.        , 0.9944009 , 0.99888018]),\n",
       " 'split2_train_score': array([0.99440716, 1.        , 0.99217002, 0.99888143]),\n",
       " 'mean_train_score': array([0.99664263, 1.        , 0.99477709, 0.99925387]),\n",
       " 'std_train_score': array([0.0018261 , 0.        , 0.0022977 , 0.00052759])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0       0.861493      0.047439         0.359999        0.008711   \n",
      "1       2.633259      0.081691         0.567231        0.033327   \n",
      "2       0.789227      0.035767         0.375109        0.027963   \n",
      "3       2.754572      0.240330         0.515534        0.103406   \n",
      "\n",
      "  param_nb__alpha param_vect__ngram_range  \\\n",
      "0            0.01                  (1, 1)   \n",
      "1            0.01                  (1, 2)   \n",
      "2             0.1                  (1, 1)   \n",
      "3             0.1                  (1, 2)   \n",
      "\n",
      "                                             params  split0_test_score  \\\n",
      "0  {'nb__alpha': 0.01, 'vect__ngram_range': (1, 1)}           0.756152   \n",
      "1  {'nb__alpha': 0.01, 'vect__ngram_range': (1, 2)}           0.744966   \n",
      "2   {'nb__alpha': 0.1, 'vect__ngram_range': (1, 1)}           0.782998   \n",
      "3   {'nb__alpha': 0.1, 'vect__ngram_range': (1, 2)}           0.758389   \n",
      "\n",
      "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
      "0           0.736018           0.786996         0.759701        0.020959   \n",
      "1           0.751678           0.784753         0.760448        0.017385   \n",
      "2           0.760626           0.798206         0.780597        0.015433   \n",
      "3           0.776286           0.798206         0.777612        0.016279   \n",
      "\n",
      "   rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0                4             0.99888            0.996641   \n",
      "1                3             1.00000            1.000000   \n",
      "2                1             0.99776            0.994401   \n",
      "3                2             1.00000            0.998880   \n",
      "\n",
      "   split2_train_score  mean_train_score  std_train_score  \n",
      "0            0.994407          0.996643         0.001826  \n",
      "1            1.000000          1.000000         0.000000  \n",
      "2            0.992170          0.994777         0.002298  \n",
      "3            0.998881          0.999254         0.000528  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(gs_clf.cv_results_)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0009f', '007', '00s', '05', '10', '100', '1000', '10000', '100m', '101', '102', '103', '104', '105', '106', '107', '108', '109', '10b', '10s', '10th', '11', '110', '111', '112', '113', '1138', '114', '115', '118', '11th', '12', '121', '125', '126', '127', '1272', '129', '1298', '12th', '13', '130', '1305', '132', '135', '137', '138', '13th']\n",
      "['ziembicki', 'ziggy', 'zigs', 'zigzagged', 'zilch', 'zillion', 'zimbabwe', 'zimmer', 'zimmerman', 'zinger', 'zingers', 'zinnia', 'zip', 'zipped', 'zippel', 'zipper', 'zippers', 'zippy', 'zit', 'zodiac', 'zoe', 'zombie', 'zombies', 'zone', 'zones', 'zoo', 'zookeeper', 'zookeepers', 'zoolander', 'zoologist', 'zoologists', 'zoom', 'zooming', 'zooms', 'zoot', 'zorg', 'zorro', 'zsigmond', 'zucker', 'zuehlke', 'zuko', 'zukovsky', 'zulu', 'zundel', 'zurg', 'zweibel', 'zwick', 'zwigoff', 'zycie', 'zzzzzzz']\n",
      "[[ 3. 49.  0. ...  1.  0.  1.]\n",
      " [ 1. 39.  1. ...  1.  2.  0.]]\n",
      "(2, 33778)\n",
      "[ 3. 49.  0. ...  1.  0.  1.]\n",
      "[ 1. 39.  1. ...  1.  2.  0.]\n",
      "        pos   neg\n",
      "token            \n",
      "00      3.0   1.0\n",
      "000    49.0  39.0\n",
      "0009f   0.0   1.0\n",
      "007     1.0   6.0\n",
      "00s     1.0   0.0\n",
      "           pos   neg\n",
      "token               \n",
      "brable     0.0   1.0\n",
      "bates      6.0  15.0\n",
      "sentenced  6.0   3.0\n",
      "elliot     1.0  19.0\n",
      "gunpoint   2.0   0.0\n",
      "[665. 675.]\n",
      "                pos       neg  neg_ratio\n",
      "token                                   \n",
      "brable     0.001504  0.002963   1.970370\n",
      "bates      0.010526  0.023704   2.251852\n",
      "sentenced  0.010526  0.005926   0.562963\n",
      "elliot     0.003008  0.029630   9.851852\n",
      "gunpoint   0.004511  0.001481   0.328395\n",
      "                   pos       neg  neg_ratio\n",
      "token                                      \n",
      "lebowski      0.001504  0.062222  41.377778\n",
      "mulan         0.003008  0.118519  39.407407\n",
      "flynt         0.003008  0.117037  38.914815\n",
      "shrek         0.001504  0.056296  37.437037\n",
      "margaret      0.001504  0.047407  31.525926\n",
      "ordell        0.001504  0.047407  31.525926\n",
      "jedi          0.003008  0.088889  29.555556\n",
      "homer         0.001504  0.044444  29.555556\n",
      "guido         0.001504  0.041481  27.585185\n",
      "gattaca       0.001504  0.041481  27.585185\n",
      "lambeau       0.001504  0.040000  26.600000\n",
      "sweetback     0.001504  0.038519  25.614815\n",
      "lama          0.001504  0.032593  21.674074\n",
      "obi           0.001504  0.032593  21.674074\n",
      "lumumba       0.001504  0.032593  21.674074\n",
      "coens         0.001504  0.031111  20.688889\n",
      "anakin        0.001504  0.031111  20.688889\n",
      "donkey        0.001504  0.031111  20.688889\n",
      "qui           0.001504  0.028148  18.718519\n",
      "dalai         0.001504  0.028148  18.718519\n",
      "hugo          0.001504  0.028148  18.718519\n",
      "booker        0.001504  0.026667  17.733333\n",
      "venice        0.001504  0.026667  17.733333\n",
      "carver        0.001504  0.026667  17.733333\n",
      "bianca        0.001504  0.026667  17.733333\n",
      "kings         0.001504  0.026667  17.733333\n",
      "gon           0.001504  0.026667  17.733333\n",
      "mib           0.001504  0.026667  17.733333\n",
      "camille       0.001504  0.026667  17.733333\n",
      "fantasia      0.001504  0.025185  16.748148\n",
      "...                ...       ...        ...\n",
      "wisecracking  0.021053  0.001481   0.070370\n",
      "tsui          0.021053  0.001481   0.070370\n",
      "liu           0.021053  0.001481   0.070370\n",
      "heyst         0.021053  0.001481   0.070370\n",
      "jill          0.021053  0.001481   0.070370\n",
      "bats          0.063158  0.004444   0.070370\n",
      "psychlo       0.022556  0.001481   0.065679\n",
      "musketeer     0.022556  0.001481   0.065679\n",
      "avengers      0.022556  0.001481   0.065679\n",
      "grinch        0.022556  0.001481   0.065679\n",
      "wcw           0.022556  0.001481   0.065679\n",
      "caulder       0.022556  0.001481   0.065679\n",
      "webb          0.046617  0.002963   0.063560\n",
      "geronimo      0.024060  0.001481   0.061574\n",
      "forlani       0.024060  0.001481   0.061574\n",
      "jericho       0.025564  0.001481   0.057952\n",
      "ivy           0.025564  0.001481   0.057952\n",
      "hush          0.027068  0.001481   0.054733\n",
      "kersey        0.027068  0.001481   0.054733\n",
      "psychlos      0.027068  0.001481   0.054733\n",
      "dalmatians    0.028571  0.001481   0.051852\n",
      "supergirl     0.028571  0.001481   0.051852\n",
      "bilko         0.030075  0.001481   0.049259\n",
      "wayans        0.030075  0.001481   0.049259\n",
      "macdonald     0.030075  0.001481   0.049259\n",
      "insulting     0.031579  0.001481   0.046914\n",
      "hudson        0.034586  0.001481   0.042834\n",
      "brenner       0.034586  0.001481   0.042834\n",
      "werewolf      0.040602  0.001481   0.036488\n",
      "nbsp          0.088722  0.001481   0.016698\n",
      "\n",
      "[33778 rows x 3 columns]\n",
      "1.0093069873997709\n"
     ]
    }
   ],
   "source": [
    "##TODO : neg are neg ? pos are pos ?\n",
    "nb = MultinomialNB()\n",
    "# store the vocabulary of X_train\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "X_train_tokens = vect.get_feature_names()\n",
    "len(X_train_tokens)\n",
    "# examine the first 50 tokens\n",
    "print(X_train_tokens[0:50])\n",
    "# examine the last 50 tokens\n",
    "print(X_train_tokens[-50:])\n",
    "# Naive Bayes counts the number of times each token appears in each class\n",
    "# trailing underscore is scikit convention for attributes that are learned during model fitting\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "nb = nb.fit(X_train_dtm, y_train)\n",
    "print(nb.feature_count_)\n",
    "# rows represent classes, columns represent tokens\n",
    "print(nb.feature_count_.shape)\n",
    "# number of times each token appears across all HAM messages\n",
    "pos_token_count = nb.feature_count_[0, :]\n",
    "print(pos_token_count)\n",
    "# number of times each token appears across all SPAM messages\n",
    "neg_token_count = nb.feature_count_[1, :]\n",
    "print(neg_token_count)\n",
    "# create a DataFrame of tokens with their separate ham and spam counts\n",
    "tokens = pd.DataFrame({\"token\":X_train_tokens, \"pos\":pos_token_count, \"neg\":neg_token_count}).set_index(\"token\")\n",
    "print(tokens.head())\n",
    "# examine 5 random DataFrame rows\n",
    "print(tokens.sample(5, random_state=6))\n",
    "# Naive Bayes counts the number of observations in each class\n",
    "print(nb.class_count_)\n",
    "# add 1 to ham and spam counts to avoid 0 probabilities\n",
    "tokens['pos'] = tokens['pos'] + 1\n",
    "tokens['neg'] = tokens['neg'] + 1\n",
    "tokens.sample(5, random_state=6)\n",
    "# convert the ham and spam counts into frequencies\n",
    "tokens['pos'] = tokens['pos'] / nb.class_count_[0]\n",
    "tokens['neg'] = tokens['neg'] / nb.class_count_[1]\n",
    "tokens.sample(5, random_state=6)\n",
    "# calculate the ratio of neg-to-pos for each token\n",
    "tokens['neg_ratio'] = tokens['neg'] / tokens['pos']\n",
    "print(tokens.sample(5, random_state=6))\n",
    "# examine the DataFrame sorted by spam_ratio\n",
    "# note: use sort() instead of sort_values() for pandas 0.16.2 and earlier\n",
    "print(tokens.sort_values('neg_ratio', ascending=False))\n",
    "# look up the spam_ratio for a given token\n",
    "print(tokens.loc[\"good\", \"neg_ratio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos       0.83      0.84      0.84       335\n",
      "         neg       0.84      0.83      0.83       325\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       660\n",
      "   macro avg       0.84      0.84      0.84       660\n",
      "weighted avg       0.84      0.84      0.84       660\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8362342135476464"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same parameter to vectorize data in order to compare to precedent methods\n",
    "# Logistic regression\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range =(1,2), max_df =0.7, min_df=2)),\n",
    "                     ('lr', LogisticRegression()),\n",
    "                    ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "y_pred_class = text_clf.predict(X_test)\n",
    "\n",
    "y_pred_prob = text_clf.predict(X_test)\n",
    "\n",
    "metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_class,\n",
    "    target_names=[\"pos\",\"neg\"]))\n",
    "metrics.accuracy_score(y_test, y_pred_class)\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [660, 1340]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-8cdad7e77d36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0my_pred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m print(metrics.classification_report(y_test, y_pred_class,\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 230\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [660, 1340]"
     ]
    }
   ],
   "source": [
    "# With TF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vect = CountVectorizer(stop_words='english', ngram_range =(1,2), max_df =0.7, min_df=2)\n",
    "\n",
    "# combine fit and transform into a single step\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "#look at the training data\n",
    "pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())\n",
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm\n",
    "\n",
    "#Compute TF\n",
    "tf_transformer = TfidfTransformer(use_idf=False)\n",
    "X_train_tf = tf_transformer.fit_transform(X_train_dtm)\n",
    "X_train_tf.shape\n",
    "X_test_tf = TfidfTransformer(use_idf=False).fit_transform(X_train_dtm)\n",
    "#Train logistic regression with TF representation\n",
    "lr = LogisticRegression().fit(X_train_tf, y_train)\n",
    "\n",
    "y_pred_class = lr.predict(X_test_tf)\n",
    "\n",
    "y_pred_prob = lr.predict_proba(X_test_tf)\n",
    "\n",
    "metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_class,\n",
    "    target_names=[\"pos\",\"neg\"]))\n",
    "metrics.accuracy_score(y_test, y_pred_class)\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Computing TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_dtm)\n",
    "X_train_tfidf.shape\n",
    "\n",
    "nb_tfidf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_dtm)\n",
    "print(X_test_tfidf.shape)\n",
    "y_pred_class_tfidf = nb_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "metrics.accuracy_score(y_test, y_pred_class_tfidf)\n",
    "\n",
    "plot_confusion_matrix(metrics.confusion_matrix(y_test, y_pred_class_tfidf),[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE STRING KERNELS : not done until now in others group and suggested at the very end of the teacher's notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        ...ki',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform'))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'knn__n_neighbors': (10, 25, 50), 'knn__p': (1, 2), 'knn__weights': ('uniform', 'distance')},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#test a range of hyperparameters\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', max_df =0.7, min_df=2)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('knn', KNeighborsClassifier()),\n",
    "                    ])\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf' : (True,False),\n",
    "              'knn__n_neighbors': (10, 25, 50),\n",
    "              'knn__p' : (1,2),\n",
    "              'knn__weights': ('uniform', 'distance'),\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "gs_clf.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn__n_neighbors: 50\n",
      "knn__p: 2\n",
      "knn__weights: 'distance'\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 2)\n",
      "0.7626865671641792\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>knn__n_neighbors</th>\n",
       "      <th>knn__p</th>\n",
       "      <th>knn__weights</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.762687</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.755970</td>\n",
       "      <td>0.767172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.753731</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.749254</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.747761</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.741791</td>\n",
       "      <td>0.769410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.738060</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.736567</td>\n",
       "      <td>0.760822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.736567</td>\n",
       "      <td>0.749627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.734328</td>\n",
       "      <td>0.743657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ngram_range  tfidf knn__n_neighbors knn__p knn__weights  mean_test_score  \\\n",
       "rank                                                                            \n",
       "1         (1, 2)   True               50      2     distance         0.762687   \n",
       "2         (1, 2)   True               50      2      uniform         0.755970   \n",
       "3         (1, 1)   True               50      2     distance         0.753731   \n",
       "4         (1, 2)  False               50      2     distance         0.749254   \n",
       "5         (1, 1)  False               50      2     distance         0.747761   \n",
       "6         (1, 1)   True               50      2      uniform         0.741791   \n",
       "7         (1, 2)  False               25      2     distance         0.738060   \n",
       "8         (1, 2)  False               25      2      uniform         0.736567   \n",
       "8         (1, 2)  False               50      2      uniform         0.736567   \n",
       "10        (1, 1)  False               50      2      uniform         0.734328   \n",
       "\n",
       "      mean_train_score  \n",
       "rank                    \n",
       "1             1.000000  \n",
       "2             0.767172  \n",
       "3             1.000000  \n",
       "4             1.000000  \n",
       "5             1.000000  \n",
       "6             0.769410  \n",
       "7             1.000000  \n",
       "8             0.760822  \n",
       "8             0.749627  \n",
       "10            0.743657  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "print(gs_clf.best_score_)\n",
    "df = pd.DataFrame({'rank':gs_clf.cv_results_['rank_test_score'], \n",
    "                  'ngram_range':gs_clf.cv_results_['param_vect__ngram_range'],\n",
    "                  'tfidf':gs_clf.cv_results_['param_tfidf__use_idf'],\n",
    "                  'knn__n_neighbors': gs_clf.cv_results_['param_knn__n_neighbors'],\n",
    "                  'knn__p': gs_clf.cv_results_['param_knn__p'], \n",
    "                  'knn__weights': gs_clf.cv_results_['param_knn__weights'], \n",
    "                  'mean_test_score':gs_clf.cv_results_['mean_test_score'], \n",
    "                  'mean_train_score':gs_clf.cv_results_['mean_train_score']}).set_index('rank')\n",
    "df.sort_values('rank',ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#test a range of hyperparameters\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', max_df =0.7, min_df=2)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('knn', RandomForestClassifier()),\n",
    "                    ])\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf' : (True,False),\n",
    "              'n_estimators': (10, 25, 50),\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "print(gs_clf.best_score_)\n",
    "df = pd.DataFrame(gs_clf.cv_results_)\n",
    "print(df.sort_values('rank',ascending=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
