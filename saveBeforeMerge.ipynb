{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from scikitplot.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>pos</td>\n",
       "      <td>while watching \" shallow grave , \" i found mys...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>pos</td>\n",
       "      <td>when i saw the trailer for this film , i laugh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>neg</td>\n",
       "      <td>\" spice world \" is just one long promotional ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>neg</td>\n",
       "      <td>i read the new yorker magazine and i enjoy som...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>pos</td>\n",
       "      <td>when i first heard that kevin costner was maki...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text  label_num\n",
       "729    pos  while watching \" shallow grave , \" i found mys...          1\n",
       "770    pos  when i saw the trailer for this film , i laugh...          1\n",
       "1944   neg   \" spice world \" is just one long promotional ...          0\n",
       "1623   neg  i read the new yorker magazine and i enjoy som...          0\n",
       "70     pos  when i first heard that kevin costner was maki...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get all files path\n",
    "posFiles = glob('review_polarity/txt_sentoken/pos/*')\n",
    "negFiles = glob('review_polarity/txt_sentoken/neg/*')\n",
    "#read text files\n",
    "posReviews = np.array([open(f).read() for f in posFiles])\n",
    "negReviews = np.array([open(f).read() for f in negFiles])\n",
    "#use pandas to label and mix the data\n",
    "polarity_files_df = pd.DataFrame({'pos':posReviews,'neg':negReviews})\n",
    "polarity_files_df = pd.melt(polarity_files_df, value_vars=['pos','neg'],value_name=\"text\",var_name=\"label\")\n",
    "polarity_files_df[\"label_num\"] = polarity_files_df.label.map({\"neg\":0, \"pos\":1})\n",
    "polarity_files_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340,)\n",
      "(660,)\n",
      "(1340,)\n",
      "(660,)\n"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(polarity_files_df.text, polarity_files_df.label_num, test_size=0.33, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos       0.80      0.78      0.79       335\n",
      "         neg       0.78      0.80      0.79       325\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       660\n",
      "   macro avg       0.79      0.79      0.79       660\n",
      "weighted avg       0.79      0.79      0.79       660\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78939393939393943"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove English stop words\n",
    "# include 1-grams and 2-grams (if 3-grams : seems not better because of the length of the vector)\n",
    "# ignore terms that appear in more than 70% of the documents (intuitively meaningful, it is indeed the best multiple of 10% to have a good score )\n",
    "# only keep terms that appear in at least 2 documents\n",
    "\n",
    "# TODO : remove 'not' etc. from stopwords to not remove 'not' from sentences like 'this film is not bad'\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range =(1,2), max_df =0.7, min_df=2)),\n",
    "                     ('nb', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "# à vérifier : pos et neg sont biens les pos et neg\n",
    "print(metrics.classification_report(y_test, y_pred,\n",
    "    target_names=[\"pos\",\"neg\"]))\n",
    "metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "#plt.figure()\n",
    "#plot_confusion_matrix(cnf_matrix, classes=['pos','neg'],\n",
    "#                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "#plt.figure()\n",
    "#plot_confusion_matrix(cnf_matrix, classes=['pos','neg'], normalize=True,\n",
    "#                      title='Normalized confusion matrix')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'nb': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " 'nb__alpha': 1.0,\n",
       " 'nb__class_prior': None,\n",
       " 'nb__fit_prior': True,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
       "           ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
       "         ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 0.7,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 2,\n",
       " 'vect__ngram_range': (1, 3),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': 'english',\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': None,\n",
       " 'vect__vocabulary': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test a range of hyperparameters\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range =(1,3), max_df =0.7, min_df=2)),\n",
    "                     ('nb', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "text_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#TO ADAPT depending on what text_clf.get_params() displays\n",
    "#how did we look for a good parameter alpha : try with 1e-3 and 1e-2 : 1e-2 is the best so we tried with 1e0 and 1e-1 and 1e-2 : 1e-1 is the best one\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'nb__alpha': (1e-2, 1e-1),\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.79552238805970155"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "\n",
    "gs_clf.best_score_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb__alpha: 0.1\n",
      "tfidf__use_idf: False\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        ...False,\n",
       "         use_idf=False)), ('nb', MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_estimator_.get_params()[\"nb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.90705363,  2.97942503,  0.81350295,  3.52606408,  0.84179568,\n",
       "         3.05628093,  0.86787113,  2.99055672,  0.8450772 ,  2.92341105,\n",
       "         0.79071728,  2.67280912]),\n",
       " 'mean_score_time': array([ 0.36827596,  0.56174795,  0.35869686,  0.54121161,  0.4139332 ,\n",
       "         0.64748732,  0.40891202,  0.57405368,  0.3593332 ,  0.57712452,\n",
       "         0.34732358,  0.47334862]),\n",
       " 'mean_test_score': array([ 0.80074627,  0.79328358,  0.82835821,  0.82014925,  0.77835821,\n",
       "         0.77537313,  0.80149254,  0.79626866,  0.76119403,  0.76343284,\n",
       "         0.78283582,  0.77835821]),\n",
       " 'mean_train_score': array([ 0.99291072,  0.9981353 ,  0.98656841,  0.99515037,  0.99776203,\n",
       "         1.        ,  0.99253787,  0.99925429,  0.99925429,  1.        ,\n",
       "         0.99776203,  1.        ]),\n",
       " 'param_nb__alpha': masked_array(data = [0.1 0.1 0.1 0.1 0.01 0.01 0.01 0.01 0.001 0.001 0.001 0.001],\n",
       "              mask = [False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_tfidf__use_idf': masked_array(data = [True True False False True True False False True True False False],\n",
       "              mask = [False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__ngram_range': masked_array(data = [(1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2)\n",
       "  (1, 1) (1, 2)],\n",
       "              mask = [False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'nb__alpha': 0.1,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'nb__alpha': 0.1, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
       "  {'nb__alpha': 0.1, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
       "  {'nb__alpha': 0.1, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)},\n",
       "  {'nb__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)},\n",
       "  {'nb__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
       "  {'nb__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
       "  {'nb__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)},\n",
       "  {'nb__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)},\n",
       "  {'nb__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
       "  {'nb__alpha': 0.001, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
       "  {'nb__alpha': 0.001, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}],\n",
       " 'rank_test_score': array([ 4,  6,  1,  2,  8, 10,  3,  5, 12, 11,  7,  8], dtype=int32),\n",
       " 'split0_test_score': array([ 0.81431767,  0.80760626,  0.8344519 ,  0.82997763,  0.80089485,\n",
       "         0.80089485,  0.81655481,  0.81879195,  0.7852349 ,  0.78747204,\n",
       "         0.80313199,  0.80089485]),\n",
       " 'split0_train_score': array([ 0.99216125,  1.        ,  0.98656215,  0.99552072,  0.99888018,\n",
       "         1.        ,  0.99216125,  1.        ,  1.        ,  1.        ,\n",
       "         0.99888018,  1.        ]),\n",
       " 'split1_test_score': array([ 0.81655481,  0.80536913,  0.85011186,  0.83221477,  0.79418345,\n",
       "         0.77628635,  0.82102908,  0.8098434 ,  0.76510067,  0.76286353,\n",
       "         0.79642058,  0.77852349]),\n",
       " 'split1_train_score': array([ 0.9944009 ,  0.99888018,  0.98992161,  0.99776036,  0.99888018,\n",
       "         1.        ,  0.9944009 ,  1.        ,  1.        ,  1.        ,\n",
       "         0.99888018,  1.        ]),\n",
       " 'split2_test_score': array([ 0.77130045,  0.76681614,  0.80044843,  0.79820628,  0.73991031,\n",
       "         0.74887892,  0.76681614,  0.76008969,  0.73318386,  0.73991031,\n",
       "         0.74887892,  0.75560538]),\n",
       " 'split2_train_score': array([ 0.99217002,  0.99552573,  0.98322148,  0.99217002,  0.99552573,\n",
       "         1.        ,  0.99105145,  0.99776286,  0.99776286,  1.        ,\n",
       "         0.99552573,  1.        ]),\n",
       " 'std_fit_time': array([ 0.05344825,  0.22817698,  0.08103768,  0.14229889,  0.06774117,\n",
       "         0.15168821,  0.05769988,  0.10017736,  0.0529942 ,  0.07123096,\n",
       "         0.01431095,  0.09777595]),\n",
       " 'std_score_time': array([ 0.01425502,  0.01814319,  0.0188923 ,  0.02524657,  0.05019106,\n",
       "         0.05670544,  0.05020673,  0.05297161,  0.0030775 ,  0.04299396,\n",
       "         0.01706694,  0.08716292]),\n",
       " 'std_test_score': array([ 0.02081809,  0.01871667,  0.02072461,  0.01552557,  0.02729431,\n",
       "         0.02124127,  0.02456054,  0.02581377,  0.0214248 ,  0.01941753,\n",
       "         0.02414039,  0.01848626]),\n",
       " 'std_train_score': array([ 0.00105372,  0.00190104,  0.00273532,  0.00229722,  0.0015813 ,\n",
       "         0.        ,  0.00139309,  0.0010546 ,  0.0010546 ,  0.        ,\n",
       "         0.0015813 ,  0.        ])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0        0.907054         0.368276         0.800746          0.992911   \n",
      "1        2.979425         0.561748         0.793284          0.998135   \n",
      "2        0.813503         0.358697         0.828358          0.986568   \n",
      "3        3.526064         0.541212         0.820149          0.995150   \n",
      "4        0.841796         0.413933         0.778358          0.997762   \n",
      "5        3.056281         0.647487         0.775373          1.000000   \n",
      "6        0.867871         0.408912         0.801493          0.992538   \n",
      "7        2.990557         0.574054         0.796269          0.999254   \n",
      "8        0.845077         0.359333         0.761194          0.999254   \n",
      "9        2.923411         0.577125         0.763433          1.000000   \n",
      "10       0.790717         0.347324         0.782836          0.997762   \n",
      "11       2.672809         0.473349         0.778358          1.000000   \n",
      "\n",
      "   param_nb__alpha param_tfidf__use_idf param_vect__ngram_range  \\\n",
      "0              0.1                 True                  (1, 1)   \n",
      "1              0.1                 True                  (1, 2)   \n",
      "2              0.1                False                  (1, 1)   \n",
      "3              0.1                False                  (1, 2)   \n",
      "4             0.01                 True                  (1, 1)   \n",
      "5             0.01                 True                  (1, 2)   \n",
      "6             0.01                False                  (1, 1)   \n",
      "7             0.01                False                  (1, 2)   \n",
      "8            0.001                 True                  (1, 1)   \n",
      "9            0.001                 True                  (1, 2)   \n",
      "10           0.001                False                  (1, 1)   \n",
      "11           0.001                False                  (1, 2)   \n",
      "\n",
      "                                               params  rank_test_score  \\\n",
      "0   {'nb__alpha': 0.1, 'vect__ngram_range': (1, 1)...                4   \n",
      "1   {'nb__alpha': 0.1, 'vect__ngram_range': (1, 2)...                6   \n",
      "2   {'nb__alpha': 0.1, 'vect__ngram_range': (1, 1)...                1   \n",
      "3   {'nb__alpha': 0.1, 'vect__ngram_range': (1, 2)...                2   \n",
      "4   {'nb__alpha': 0.01, 'vect__ngram_range': (1, 1...                8   \n",
      "5   {'nb__alpha': 0.01, 'vect__ngram_range': (1, 2...               10   \n",
      "6   {'nb__alpha': 0.01, 'vect__ngram_range': (1, 1...                3   \n",
      "7   {'nb__alpha': 0.01, 'vect__ngram_range': (1, 2...                5   \n",
      "8   {'nb__alpha': 0.001, 'vect__ngram_range': (1, ...               12   \n",
      "9   {'nb__alpha': 0.001, 'vect__ngram_range': (1, ...               11   \n",
      "10  {'nb__alpha': 0.001, 'vect__ngram_range': (1, ...                7   \n",
      "11  {'nb__alpha': 0.001, 'vect__ngram_range': (1, ...                8   \n",
      "\n",
      "    split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0            0.814318            0.992161           0.816555   \n",
      "1            0.807606            1.000000           0.805369   \n",
      "2            0.834452            0.986562           0.850112   \n",
      "3            0.829978            0.995521           0.832215   \n",
      "4            0.800895            0.998880           0.794183   \n",
      "5            0.800895            1.000000           0.776286   \n",
      "6            0.816555            0.992161           0.821029   \n",
      "7            0.818792            1.000000           0.809843   \n",
      "8            0.785235            1.000000           0.765101   \n",
      "9            0.787472            1.000000           0.762864   \n",
      "10           0.803132            0.998880           0.796421   \n",
      "11           0.800895            1.000000           0.778523   \n",
      "\n",
      "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0             0.994401           0.771300            0.992170      0.053448   \n",
      "1             0.998880           0.766816            0.995526      0.228177   \n",
      "2             0.989922           0.800448            0.983221      0.081038   \n",
      "3             0.997760           0.798206            0.992170      0.142299   \n",
      "4             0.998880           0.739910            0.995526      0.067741   \n",
      "5             1.000000           0.748879            1.000000      0.151688   \n",
      "6             0.994401           0.766816            0.991051      0.057700   \n",
      "7             1.000000           0.760090            0.997763      0.100177   \n",
      "8             1.000000           0.733184            0.997763      0.052994   \n",
      "9             1.000000           0.739910            1.000000      0.071231   \n",
      "10            0.998880           0.748879            0.995526      0.014311   \n",
      "11            1.000000           0.755605            1.000000      0.097776   \n",
      "\n",
      "    std_score_time  std_test_score  std_train_score  \n",
      "0         0.014255        0.020818         0.001054  \n",
      "1         0.018143        0.018717         0.001901  \n",
      "2         0.018892        0.020725         0.002735  \n",
      "3         0.025247        0.015526         0.002297  \n",
      "4         0.050191        0.027294         0.001581  \n",
      "5         0.056705        0.021241         0.000000  \n",
      "6         0.050207        0.024561         0.001393  \n",
      "7         0.052972        0.025814         0.001055  \n",
      "8         0.003077        0.021425         0.001055  \n",
      "9         0.042994        0.019418         0.000000  \n",
      "10        0.017067        0.024140         0.001581  \n",
      "11        0.087163        0.018486         0.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(gs_clf.cv_results_)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '007', '00s', '03', '04', '05', '05425', '10', '100', '1000', '100m', '101', '102', '103', '104', '105', '106', '107', '108', '109', '10b', '10s', '10th', '11', '110', '111', '112', '113', '114', '115', '117', '118', '11th', '12', '123', '125', '126', '127', '1272', '128', '129', '1298', '12th', '13', '130', '1305', '131', '132', '133']\n",
      "['zi', 'zidler', 'ziegler', 'ziembicki', 'zigged', 'ziggy', 'zilch', 'zimbabwe', 'zimmely', 'zimmer', 'zimmerly', 'zinger', 'zingers', 'zinnia', 'zip', 'zipped', 'zippel', 'zipper', 'zippers', 'zippy', 'zips', 'ziyi', 'zodiac', 'zoe', 'zombie', 'zombies', 'zombified', 'zone', 'zones', 'zoo', 'zookeeper', 'zoolander', 'zoologist', 'zoom', 'zooming', 'zooms', 'zoot', 'zophres', 'zorg', 'zorro', 'zsigmond', 'zucker', 'zuehlke', 'zuko', 'zukovsky', 'zulu', 'zwick', 'zwigoff', 'zycie', 'zzzzzzz']\n",
      "[[  5.  34.   3. ...,   0.   0.   1.]\n",
      " [  1.  33.   6. ...,   1.   2.   0.]]\n",
      "(2, 34197)\n",
      "[  5.  34.   3. ...,   0.   0.   1.]\n",
      "[  1.  33.   6. ...,   1.   2.   0.]\n",
      "        neg   pos\n",
      "token            \n",
      "00      1.0   5.0\n",
      "000    33.0  34.0\n",
      "007     6.0   3.0\n",
      "00s     0.0   1.0\n",
      "03      0.0   2.0\n",
      "                 neg  pos\n",
      "token                    \n",
      "psyches          1.0  0.0\n",
      "finnegan         0.0  3.0\n",
      "trustworthiness  1.0  0.0\n",
      "patton           2.0  2.0\n",
      "salivate         0.0  1.0\n",
      "[ 665.  675.]\n",
      "                      neg       pos  neg_ratio\n",
      "token                                         \n",
      "psyches          0.002963  0.001504   1.970370\n",
      "finnegan         0.001481  0.006015   0.246296\n",
      "trustworthiness  0.002963  0.001504   1.970370\n",
      "patton           0.004444  0.004511   0.985185\n",
      "salivate         0.001481  0.003008   0.492593\n",
      "                    neg       pos  neg_ratio\n",
      "token                                       \n",
      "mulan          0.142222  0.001504  94.577778\n",
      "flynt          0.118519  0.001504  78.814815\n",
      "sweetback      0.045926  0.001504  30.540741\n",
      "ordell         0.044444  0.001504  29.555556\n",
      "hedwig         0.042963  0.001504  28.570370\n",
      "argento        0.041481  0.001504  27.585185\n",
      "taran          0.040000  0.001504  26.600000\n",
      "pleasantville  0.038519  0.001504  25.614815\n",
      "lambeau        0.038519  0.001504  25.614815\n",
      "fei            0.038519  0.001504  25.614815\n",
      "lebowski       0.075556  0.003008  25.122222\n",
      "chad           0.037037  0.001504  24.629630\n",
      "mallory        0.035556  0.001504  23.644444\n",
      "matilda        0.034074  0.001504  22.659259\n",
      "rounders       0.032593  0.001504  21.674074\n",
      "carver         0.032593  0.001504  21.674074\n",
      "lumumba        0.032593  0.001504  21.674074\n",
      "redford        0.031111  0.001504  20.688889\n",
      "rico           0.031111  0.001504  20.688889\n",
      "cauldron       0.062222  0.003008  20.688889\n",
      "shrek          0.029630  0.001504  19.703704\n",
      "maximus        0.028148  0.001504  18.718519\n",
      "dolores        0.028148  0.001504  18.718519\n",
      "bubby          0.028148  0.001504  18.718519\n",
      "capone         0.026667  0.001504  17.733333\n",
      "gale           0.026667  0.001504  17.733333\n",
      "hen            0.026667  0.001504  17.733333\n",
      "bianca         0.026667  0.001504  17.733333\n",
      "motta          0.026667  0.001504  17.733333\n",
      "damon          0.053333  0.003008  17.733333\n",
      "...                 ...       ...        ...\n",
      "silverman      0.001481  0.021053   0.070370\n",
      "zach           0.001481  0.021053   0.070370\n",
      "bont           0.001481  0.021053   0.070370\n",
      "vikings        0.001481  0.021053   0.070370\n",
      "silverstone    0.001481  0.021053   0.070370\n",
      "sphere         0.001481  0.021053   0.070370\n",
      "dwayne         0.001481  0.021053   0.070370\n",
      "grinch         0.001481  0.022556   0.065679\n",
      "caulder        0.001481  0.022556   0.065679\n",
      "musketeer      0.001481  0.022556   0.065679\n",
      "webb           0.002963  0.046617   0.063560\n",
      "jericho        0.001481  0.024060   0.061574\n",
      "brenner        0.001481  0.024060   0.061574\n",
      "macdonald      0.001481  0.025564   0.057952\n",
      "psychlo        0.001481  0.025564   0.057952\n",
      "bilko          0.001481  0.025564   0.057952\n",
      "sinise         0.001481  0.027068   0.054733\n",
      "eszterhas      0.001481  0.027068   0.054733\n",
      "mandingo       0.001481  0.027068   0.054733\n",
      "kersey         0.001481  0.027068   0.054733\n",
      "bronson        0.001481  0.027068   0.054733\n",
      "alicia         0.002963  0.057143   0.051852\n",
      "schumacher     0.002963  0.061654   0.048058\n",
      "hewitt         0.001481  0.034586   0.042834\n",
      "crawford       0.001481  0.036090   0.041049\n",
      "freddie        0.001481  0.040602   0.036488\n",
      "jakob          0.001481  0.042105   0.035185\n",
      "prinze         0.001481  0.042105   0.035185\n",
      "seagal         0.002963  0.094737   0.031276\n",
      "nbsp           0.001481  0.088722   0.016698\n",
      "\n",
      "[34197 rows x 3 columns]\n",
      "1.0544946957\n"
     ]
    }
   ],
   "source": [
    "##TODO : neg are neg ? pos are pos ?\n",
    "nb = MultinomialNB()\n",
    "# store the vocabulary of X_train\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "X_train_tokens = vect.get_feature_names()\n",
    "len(X_train_tokens)\n",
    "# examine the first 50 tokens\n",
    "print(X_train_tokens[0:50])\n",
    "# examine the last 50 tokens\n",
    "print(X_train_tokens[-50:])\n",
    "# Naive Bayes counts the number of times each token appears in each class\n",
    "# trailing underscore is scikit convention for attributes that are learned during model fitting\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "nb = nb.fit(X_train_dtm, y_train)\n",
    "print(nb.feature_count_)\n",
    "# rows represent classes, columns represent tokens\n",
    "print(nb.feature_count_.shape)\n",
    "# number of times each token appears across all HAM messages\n",
    "pos_token_count = nb.feature_count_[0, :]\n",
    "print(pos_token_count)\n",
    "# number of times each token appears across all SPAM messages\n",
    "neg_token_count = nb.feature_count_[1, :]\n",
    "print(neg_token_count)\n",
    "# create a DataFrame of tokens with their separate ham and spam counts\n",
    "tokens = pd.DataFrame({\"token\":X_train_tokens, \"pos\":pos_token_count, \"neg\":neg_token_count}).set_index(\"token\")\n",
    "print(tokens.head())\n",
    "# examine 5 random DataFrame rows\n",
    "print(tokens.sample(5, random_state=6))\n",
    "# Naive Bayes counts the number of observations in each class\n",
    "print(nb.class_count_)\n",
    "# add 1 to ham and spam counts to avoid 0 probabilities\n",
    "tokens['pos'] = tokens['pos'] + 1\n",
    "tokens['neg'] = tokens['neg'] + 1\n",
    "tokens.sample(5, random_state=6)\n",
    "# convert the ham and spam counts into frequencies\n",
    "tokens['pos'] = tokens['pos'] / nb.class_count_[0]\n",
    "tokens['neg'] = tokens['neg'] / nb.class_count_[1]\n",
    "tokens.sample(5, random_state=6)\n",
    "# calculate the ratio of neg-to-pos for each token\n",
    "tokens['neg_ratio'] = tokens['neg'] / tokens['pos']\n",
    "print(tokens.sample(5, random_state=6))\n",
    "# examine the DataFrame sorted by spam_ratio\n",
    "# note: use sort() instead of sort_values() for pandas 0.16.2 and earlier\n",
    "print(tokens.sort_values('neg_ratio', ascending=False))\n",
    "# look up the spam_ratio for a given token\n",
    "print(tokens.loc[\"good\", \"neg_ratio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same parameter to vectorize data in order to compare to precedent methods\n",
    "# Logistic regression\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range =(1,2), max_df =0.7, min_df=2)),\n",
    "                     ('nb', LogisticRegression()),\n",
    "                    ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "y_pred_class = text_clf.predict(X_test)\n",
    "\n",
    "y_pred_prob = text_clf.predict(X_test)\n",
    "\n",
    "metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_class,\n",
    "    target_names=[\"pos\",\"neg\"]))\n",
    "metrics.accuracy_score(y_test, y_pred_class)\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5aced182d9df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mX_train_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_dtm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mX_train_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mX_test_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_dtm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#Train logistic regression with TF representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "# With TF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vect = CountVectorizer(stop_words='english', ngram_range =(1,2), max_df =0.7, min_df=2)\n",
    "\n",
    "# combine fit and transform into a single step\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "#look at the training data\n",
    "pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())\n",
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm\n",
    "\n",
    "#Compute TF\n",
    "tf_transformer = TfidfTransformer(use_idf=False)\n",
    "X_train_tf = tf_transformer.fit_transform(X_train_dtm)\n",
    "X_train_tf.shape\n",
    "X_test_tf = TfidTransformer(use_idf=False).fit_transform(X_train_dtm)\n",
    "#Train logistic regression with TF representation\n",
    "lr = LogisticRegression().fit(X_train_tf, y_train)\n",
    "\n",
    "y_pred_class = lr.predict(X_test_tf)\n",
    "\n",
    "y_pred_prob = lr.predict_proba(X_test_tf)\n",
    "\n",
    "metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_class,\n",
    "    target_names=[\"pos\",\"neg\"]))\n",
    "metrics.accuracy_score(y_test, y_pred_class)\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(660, 50456)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-7c07a568b88d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_class_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_class_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scikitplot/metrics.py\u001b[0m in \u001b[0;36mplot_confusion_matrix\u001b[0;34m(y_true, y_pred, labels, true_labels, pred_labels, title, normalize, hide_zeros, hide_counts, x_tick_rotation, ax, figsize, cmap, title_fontsize, text_fontsize)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 230\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2, 0]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.colors' has no attribute 'to_rgba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     37\u001b[0m             display(\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             )\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36m_fetch_figure_metadata\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;34m\"\"\"Get some metadata to help with displaying a figure.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;31m# determine if a background is needed for legibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_facecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;31m# the background is transparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         ticksLight = _is_light([label.get_color()\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36m_is_transparent\u001b[0;34m(color)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;34m\"\"\"Determine transparency from alpha.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrgba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.colors' has no attribute 'to_rgba'"
     ]
    }
   ],
   "source": [
    "\n",
    "### Computing TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_dtm)\n",
    "X_train_tfidf.shape\n",
    "\n",
    "nb_tfidf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_dtm)\n",
    "print(X_test_tfidf.shape)\n",
    "y_pred_class_tfidf = nb_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "metrics.accuracy_score(y_test, y_pred_class_tfidf)\n",
    "\n",
    "plot_confusion_matrix(metrics.confusion_matrix(y_test, y_pred_class_tfidf),[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE STRING KERNELS : not done until now in others group and suggested at the very end of the teacher's notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        ...ki',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform'))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'knn__n_neighbors': (10, 25, 50), 'tfidf__use_idf': (True, False), 'vect__ngram_range': [(1, 1), (1, 2)], 'knn__weights': ('uniform', 'distance'), 'knn__p': (1, 2)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#test a range of hyperparameters\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', max_df =0.7, min_df=2)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('knn', KNeighborsClassifier()),\n",
    "                    ])\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf' : (True,False),\n",
    "              'knn__n_neighbors': (10, 25, 50),\n",
    "              'knn__p' : (1,2),\n",
    "              'knn__weights': ('uniform', 'distance'),\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "gs_clf.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn__n_neighbors: 50\n",
      "knn__p: 2\n",
      "knn__weights: 'distance'\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 1)\n",
      "0.764179104478\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-cf35602fc9d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rank'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position)\u001b[0m\n\u001b[1;32m   4419\u001b[0m             \u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4420\u001b[0m             k = self._get_label_or_level_values(by, axis=axis,\n\u001b[0;32m-> 4421\u001b[0;31m                                                 stacklevel=stacklevel)\n\u001b[0m\u001b[1;32m   4422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4423\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis, stacklevel)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rank'"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "print(gs_clf.best_score_)\n",
    "df = pd.DataFrame(gs_clf.cv_results_)\n",
    "print(df.sort_values('rank',ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#test a range of hyperparameters\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', max_df =0.7, min_df=2)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('knn', RandomForestClassifier()),\n",
    "                    ])\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf' : (True,False),\n",
    "              'n_estimators': (10, 25, 50),\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "gs_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "Jupyter Notebook\n",
    "saveBeforeMerge (auto-sauvegardé) Current Kernel Logo \n",
    "\n",
    "Python 3\n",
    "\n",
    "    Fichier\n",
    "    Édition\n",
    "    Affichage\n",
    "    Insérer\n",
    "    Cellule\n",
    "    Noyau\n",
    "    Widgets\n",
    "    Aide\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from scikitplot.metrics import plot_confusion_matrix\n",
    "\n",
    "/usr/local/lib/python3.5/dist-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
    "  _nan_object_mask = _nan_object_array != _nan_object_array\n",
    "\n",
    "​\n",
    "\n",
    "#get all files path\n",
    "\n",
    "posFiles = glob('review_polarity/txt_sentoken/pos/*')\n",
    "\n",
    "negFiles = glob('review_polarity/txt_sentoken/neg/*')\n",
    "\n",
    "#read text files\n",
    "\n",
    "posReviews = np.array([open(f).read() for f in posFiles])\n",
    "\n",
    "negReviews = np.array([open(f).read() for f in negFiles])\n",
    "\n",
    "#use pandas to label and mix the data\n",
    "\n",
    "polarity_files_df = pd.DataFrame({'pos':posReviews,'neg':negReviews})\n",
    "\n",
    "polarity_files_df = pd.melt(polarity_files_df, value_vars=['pos','neg'],value_name=\"text\",var_name=\"label\")\n",
    "\n",
    "polarity_files_df[\"label_num\"] = polarity_files_df.label.map({\"neg\":0, \"pos\":1})\n",
    "\n",
    "polarity_files_df.sample(5)\n",
    "\n",
    "\tlabel \ttext \tlabel_num\n",
    "729 \tpos \twhile watching \" shallow grave , \" i found mys... \t1\n",
    "770 \tpos \twhen i saw the trailer for this film , i laugh... \t1\n",
    "1944 \tneg \t\" spice world \" is just one long promotional ... \t0\n",
    "1623 \tneg \ti read the new yorker magazine and i enjoy som... \t0\n",
    "70 \tpos \twhen i first heard that kevin costner was maki... \t1\n",
    "\n",
    "# split X and y into training and testing sets\n",
    "\n",
    "​\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(polarity_files_df.text, polarity_files_df.label_num, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "print(y_test.shape)\n",
    "\n",
    "(1340,)\n",
    "(660,)\n",
    "(1340,)\n",
    "(660,)\n",
    "\n",
    "# remove English stop words\n",
    "\n",
    "# include 1-grams and 2-grams (if 3-grams : seems not better because of the length of the vector)\n",
    "\n",
    "# ignore terms that appear in more than 70% of the documents (intuitively meaningful, it is indeed the best multiple of 10% to have a good score )\n",
    "\n",
    "# only keep terms that appear in at least 2 documents\n",
    "\n",
    "​\n",
    "\n",
    "# TODO : remove 'not' etc. from stopwords to not remove 'not' from sentences like 'this film is not bad'\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range =(1,2), max_df =0.7, min_df=2)),\n",
    "\n",
    "                     ('nb', MultinomialNB()),\n",
    "\n",
    "                    ])\n",
    "\n",
    "​\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "​\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# à vérifier : pos et neg sont biens les pos et neg\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred,\n",
    "\n",
    "    target_names=[\"pos\",\"neg\"]))\n",
    "\n",
    "metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "​\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "\n",
    "#plt.figure()\n",
    "\n",
    "#plot_confusion_matrix(cnf_matrix, classes=['pos','neg'],\n",
    "\n",
    "#                      title='Confusion matrix, without normalization')\n",
    "\n",
    "​\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "\n",
    "#plt.figure()\n",
    "\n",
    "#plot_confusion_matrix(cnf_matrix, classes=['pos','neg'], normalize=True,\n",
    "\n",
    "#                      title='Normalized confusion matrix')\n",
    "\n",
    "​\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         pos       0.80      0.78      0.79       335\n",
    "         neg       0.78      0.80      0.79       325\n",
    "\n",
    "   micro avg       0.79      0.79      0.79       660\n",
    "   macro avg       0.79      0.79      0.79       660\n",
    "weighted avg       0.79      0.79      0.79       660\n",
    "\n",
    "0.78939393939393943\n",
    "\n",
    "​\n",
    "\n",
    "#test a range of hyperparameters\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range =(1,3), max_df =0.7, min_df=2)),\n",
    "\n",
    "                     ('nb', MultinomialNB()),\n",
    "\n",
    "                    ])\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "text_clf.get_params()\n",
    "\n",
    "{'memory': None,\n",
    " 'nb': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
    " 'nb__alpha': 1.0,\n",
    " 'nb__class_prior': None,\n",
    " 'nb__fit_prior': True,\n",
    " 'steps': [('vect',\n",
    "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
    "           lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
    "           ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
    "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "           tokenizer=None, vocabulary=None)),\n",
    "  ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
    " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
    "         lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
    "         ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
    "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "         tokenizer=None, vocabulary=None),\n",
    " 'vect__analyzer': 'word',\n",
    " 'vect__binary': False,\n",
    " 'vect__decode_error': 'strict',\n",
    " 'vect__dtype': numpy.int64,\n",
    " 'vect__encoding': 'utf-8',\n",
    " 'vect__input': 'content',\n",
    " 'vect__lowercase': True,\n",
    " 'vect__max_df': 0.7,\n",
    " 'vect__max_features': None,\n",
    " 'vect__min_df': 2,\n",
    " 'vect__ngram_range': (1, 3),\n",
    " 'vect__preprocessor': None,\n",
    " 'vect__stop_words': 'english',\n",
    " 'vect__strip_accents': None,\n",
    " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    " 'vect__tokenizer': None,\n",
    " 'vect__vocabulary': None}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#TO ADAPT depending on what text_clf.get_params() displays\n",
    "\n",
    "#how did we look for a good parameter alpha : try with 1e-3 and 1e-2 : 1e-2 is the best so we tried with 1e0 and 1e-1 and 1e-2 : 1e-1 is the best one\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "\n",
    "              'nb__alpha': (1e-2, 1e-1),\n",
    "\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "\n",
    "​\n",
    "\n",
    "gs_clf.best_score_  \n",
    "\n",
    "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
    "  warnings.warn(CV_WARNING, FutureWarning)\n",
    "\n",
    "0.79552238805970155\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "\n",
    "nb__alpha: 0.1\n",
    "tfidf__use_idf: False\n",
    "vect__ngram_range: (1, 1)\n",
    "\n",
    "gs_clf.best_estimator_\n",
    "\n",
    "Pipeline(memory=None,\n",
    "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
    "        lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
    "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
    "        ...False,\n",
    "         use_idf=False)), ('nb', MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))])\n",
    "\n",
    "gs_clf.best_estimator_.get_params()[\"nb\"]\n",
    "\n",
    "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)\n",
    "\n",
    "gs_clf.cv_results_\n",
    "\n",
    "{'mean_fit_time': array([ 0.90705363,  2.97942503,  0.81350295,  3.52606408,  0.84179568,\n",
    "         3.05628093,  0.86787113,  2.99055672,  0.8450772 ,  2.92341105,\n",
    "         0.79071728,  2.67280912]),\n",
    " 'mean_score_time': array([ 0.36827596,  0.56174795,  0.35869686,  0.54121161,  0.4139332 ,\n",
    "         0.64748732,  0.40891202,  0.57405368,  0.3593332 ,  0.57712452,\n",
    "         0.34732358,  0.47334862]),\n",
    " 'mean_test_score': array([ 0.80074627,  0.79328358,  0.82835821,  0.82014925,  0.77835821,\n",
    "         0.77537313,  0.80149254,  0.79626866,  0.76119403,  0.76343284,\n",
    "         0.78283582,  0.77835821]),\n",
    " 'mean_train_score': array([ 0.99291072,  0.9981353 ,  0.98656841,  0.99515037,  0.99776203,\n",
    "         1.        ,  0.99253787,  0.99925429,  0.99925429,  1.        ,\n",
    "         0.99776203,  1.        ]),\n",
    " 'param_nb__alpha': masked_array(data = [0.1 0.1 0.1 0.1 0.01 0.01 0.01 0.01 0.001 0.001 0.001 0.001],\n",
    "              mask = [False False False False False False False False False False False False],\n",
    "        fill_value = ?),\n",
    " 'param_tfidf__use_idf': masked_array(data = [True True False False True True False False True True False False],\n",
    "              mask = [False False False False False False False False False False False False],\n",
    "        fill_value = ?),\n",
    " 'param_vect__ngram_range': masked_array(data = [(1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2)\n",
    "  (1, 1) (1, 2)],\n",
    "              mask = [False False False False False False False False False False False False],\n",
    "        fill_value = ?),\n",
    " 'params': [{'nb__alpha': 0.1,\n",
    "   'tfidf__use_idf': True,\n",
    "   'vect__ngram_range': (1, 1)},\n",
    "  {'nb__alpha': 0.1, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
    "  {'nb__alpha': 0.1, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
    "  {'nb__alpha': 0.1, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)},\n",
    "  {'nb__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)},\n",
    "  {'nb__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
    "  {'nb__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
    "  {'nb__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)},\n",
    "  {'nb__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)},\n",
    "  {'nb__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
    "  {'nb__alpha': 0.001, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
    "  {'nb__alpha': 0.001, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}],\n",
    " 'rank_test_score': array([ 4,  6,  1,  2,  8, 10,  3,  5, 12, 11,  7,  8], dtype=int32),\n",
    " 'split0_test_score': array([ 0.81431767,  0.80760626,  0.8344519 ,  0.82997763,  0.80089485,\n",
    "         0.80089485,  0.81655481,  0.81879195,  0.7852349 ,  0.78747204,\n",
    "         0.80313199,  0.80089485]),\n",
    " 'split0_train_score': array([ 0.99216125,  1.        ,  0.98656215,  0.99552072,  0.99888018,\n",
    "         1.        ,  0.99216125,  1.        ,  1.        ,  1.        ,\n",
    "         0.99888018,  1.        ]),\n",
    " 'split1_test_score': array([ 0.81655481,  0.80536913,  0.85011186,  0.83221477,  0.79418345,\n",
    "         0.77628635,  0.82102908,  0.8098434 ,  0.76510067,  0.76286353,\n",
    "         0.79642058,  0.77852349]),\n",
    " 'split1_train_score': array([ 0.9944009 ,  0.99888018,  0.98992161,  0.99776036,  0.99888018,\n",
    "         1.        ,  0.9944009 ,  1.        ,  1.        ,  1.        ,\n",
    "         0.99888018,  1.        ]),\n",
    " 'split2_test_score': array([ 0.77130045,  0.76681614,  0.80044843,  0.79820628,  0.73991031,\n",
    "         0.74887892,  0.76681614,  0.76008969,  0.73318386,  0.73991031,\n",
    "         0.74887892,  0.75560538]),\n",
    " 'split2_train_score': array([ 0.99217002,  0.99552573,  0.98322148,  0.99217002,  0.99552573,\n",
    "         1.        ,  0.99105145,  0.99776286,  0.99776286,  1.        ,\n",
    "         0.99552573,  1.        ]),\n",
    " 'std_fit_time': array([ 0.05344825,  0.22817698,  0.08103768,  0.14229889,  0.06774117,\n",
    "         0.15168821,  0.05769988,  0.10017736,  0.0529942 ,  0.07123096,\n",
    "         0.01431095,  0.09777595]),\n",
    " 'std_score_time': array([ 0.01425502,  0.01814319,  0.0188923 ,  0.02524657,  0.05019106,\n",
    "         0.05670544,  0.05020673,  0.05297161,  0.0030775 ,  0.04299396,\n",
    "         0.01706694,  0.08716292]),\n",
    " 'std_test_score': array([ 0.02081809,  0.01871667,  0.02072461,  0.01552557,  0.02729431,\n",
    "         0.02124127,  0.02456054,  0.02581377,  0.0214248 ,  0.01941753,\n",
    "         0.02414039,  0.01848626]),\n",
    " 'std_train_score': array([ 0.00105372,  0.00190104,  0.00273532,  0.00229722,  0.0015813 ,\n",
    "         0.        ,  0.00139309,  0.0010546 ,  0.0010546 ,  0.        ,\n",
    "         0.0015813 ,  0.        ])}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(gs_clf.cv_results_)\n",
    "\n",
    "print(df)\n",
    "\n",
    "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
    "0        0.907054         0.368276         0.800746          0.992911   \n",
    "1        2.979425         0.561748         0.793284          0.998135   \n",
    "2        0.813503         0.358697         0.828358          0.986568   \n",
    "3        3.526064         0.541212         0.820149          0.995150   \n",
    "4        0.841796         0.413933         0.778358          0.997762   \n",
    "5        3.056281         0.647487         0.775373          1.000000   \n",
    "6        0.867871         0.408912         0.801493          0.992538   \n",
    "7        2.990557         0.574054         0.796269          0.999254   \n",
    "8        0.845077         0.359333         0.761194          0.999254   \n",
    "9        2.923411         0.577125         0.763433          1.000000   \n",
    "10       0.790717         0.347324         0.782836          0.997762   \n",
    "11       2.672809         0.473349         0.778358          1.000000   \n",
    "\n",
    "   param_nb__alpha param_tfidf__use_idf param_vect__ngram_range  \\\n",
    "0              0.1                 True                  (1, 1)   \n",
    "1              0.1                 True                  (1, 2)   \n",
    "2              0.1                False                  (1, 1)   \n",
    "3              0.1                False                  (1, 2)   \n",
    "4             0.01                 True                  (1, 1)   \n",
    "5             0.01                 True                  (1, 2)   \n",
    "6             0.01                False                  (1, 1)   \n",
    "7             0.01                False                  (1, 2)   \n",
    "8            0.001                 True                  (1, 1)   \n",
    "9            0.001                 True                  (1, 2)   \n",
    "10           0.001                False                  (1, 1)   \n",
    "11           0.001                False                  (1, 2)   \n",
    "\n",
    "                                               params  rank_test_score  \\\n",
    "0   {'nb__alpha': 0.1, 'vect__ngram_range': (1, 1)...                4   \n",
    "1   {'nb__alpha': 0.1, 'vect__ngram_range': (1, 2)...                6   \n",
    "2   {'nb__alpha': 0.1, 'vect__ngram_range': (1, 1)...                1   \n",
    "3   {'nb__alpha': 0.1, 'vect__ngram_range': (1, 2)...                2   \n",
    "4   {'nb__alpha': 0.01, 'vect__ngram_range': (1, 1...                8   \n",
    "5   {'nb__alpha': 0.01, 'vect__ngram_range': (1, 2...               10   \n",
    "6   {'nb__alpha': 0.01, 'vect__ngram_range': (1, 1...                3   \n",
    "7   {'nb__alpha': 0.01, 'vect__ngram_range': (1, 2...                5   \n",
    "8   {'nb__alpha': 0.001, 'vect__ngram_range': (1, ...               12   \n",
    "9   {'nb__alpha': 0.001, 'vect__ngram_range': (1, ...               11   \n",
    "10  {'nb__alpha': 0.001, 'vect__ngram_range': (1, ...                7   \n",
    "11  {'nb__alpha': 0.001, 'vect__ngram_range': (1, ...                8   \n",
    "\n",
    "    split0_test_score  split0_train_score  split1_test_score  \\\n",
    "0            0.814318            0.992161           0.816555   \n",
    "1            0.807606            1.000000           0.805369   \n",
    "2            0.834452            0.986562           0.850112   \n",
    "3            0.829978            0.995521           0.832215   \n",
    "4            0.800895            0.998880           0.794183   \n",
    "5            0.800895            1.000000           0.776286   \n",
    "6            0.816555            0.992161           0.821029   \n",
    "7            0.818792            1.000000           0.809843   \n",
    "8            0.785235            1.000000           0.765101   \n",
    "9            0.787472            1.000000           0.762864   \n",
    "10           0.803132            0.998880           0.796421   \n",
    "11           0.800895            1.000000           0.778523   \n",
    "\n",
    "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
    "0             0.994401           0.771300            0.992170      0.053448   \n",
    "1             0.998880           0.766816            0.995526      0.228177   \n",
    "2             0.989922           0.800448            0.983221      0.081038   \n",
    "3             0.997760           0.798206            0.992170      0.142299   \n",
    "4             0.998880           0.739910            0.995526      0.067741   \n",
    "5             1.000000           0.748879            1.000000      0.151688   \n",
    "6             0.994401           0.766816            0.991051      0.057700   \n",
    "7             1.000000           0.760090            0.997763      0.100177   \n",
    "8             1.000000           0.733184            0.997763      0.052994   \n",
    "9             1.000000           0.739910            1.000000      0.071231   \n",
    "10            0.998880           0.748879            0.995526      0.014311   \n",
    "11            1.000000           0.755605            1.000000      0.097776   \n",
    "\n",
    "    std_score_time  std_test_score  std_train_score  \n",
    "0         0.014255        0.020818         0.001054  \n",
    "1         0.018143        0.018717         0.001901  \n",
    "2         0.018892        0.020725         0.002735  \n",
    "3         0.025247        0.015526         0.002297  \n",
    "4         0.050191        0.027294         0.001581  \n",
    "5         0.056705        0.021241         0.000000  \n",
    "6         0.050207        0.024561         0.001393  \n",
    "7         0.052972        0.025814         0.001055  \n",
    "8         0.003077        0.021425         0.001055  \n",
    "9         0.042994        0.019418         0.000000  \n",
    "10        0.017067        0.024140         0.001581  \n",
    "11        0.087163        0.018486         0.000000  \n",
    "\n",
    "##TODO : neg are neg ? pos are pos ?\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# store the vocabulary of X_train\n",
    "\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "\n",
    "X_train_tokens = vect.get_feature_names()\n",
    "\n",
    "len(X_train_tokens)\n",
    "\n",
    "# examine the first 50 tokens\n",
    "\n",
    "print(X_train_tokens[0:50])\n",
    "\n",
    "# examine the last 50 tokens\n",
    "\n",
    "print(X_train_tokens[-50:])\n",
    "\n",
    "# Naive Bayes counts the number of times each token appears in each class\n",
    "\n",
    "# trailing underscore is scikit convention for attributes that are learned during model fitting\n",
    "\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "\n",
    "nb = nb.fit(X_train_dtm, y_train)\n",
    "\n",
    "print(nb.feature_count_)\n",
    "\n",
    "# rows represent classes, columns represent tokens\n",
    "\n",
    "print(nb.feature_count_.shape)\n",
    "\n",
    "# number of times each token appears across all HAM messages\n",
    "\n",
    "pos_token_count = nb.feature_count_[0, :]\n",
    "\n",
    "print(pos_token_count)\n",
    "\n",
    "# number of times each token appears across all SPAM messages\n",
    "\n",
    "neg_token_count = nb.feature_count_[1, :]\n",
    "\n",
    "print(neg_token_count)\n",
    "\n",
    "# create a DataFrame of tokens with their separate ham and spam counts\n",
    "\n",
    "tokens = pd.DataFrame({\"token\":X_train_tokens, \"pos\":pos_token_count, \"neg\":neg_token_count}).set_index(\"token\")\n",
    "\n",
    "print(tokens.head())\n",
    "\n",
    "# examine 5 random DataFrame rows\n",
    "\n",
    "print(tokens.sample(5, random_state=6))\n",
    "\n",
    "# Naive Bayes counts the number of observations in each class\n",
    "\n",
    "print(nb.class_count_)\n",
    "\n",
    "# add 1 to ham and spam counts to avoid 0 probabilities\n",
    "\n",
    "tokens['pos'] = tokens['pos'] + 1\n",
    "\n",
    "tokens['neg'] = tokens['neg'] + 1\n",
    "\n",
    "tokens.sample(5, random_state=6)\n",
    "\n",
    "# convert the ham and spam counts into frequencies\n",
    "\n",
    "tokens['pos'] = tokens['pos'] / nb.class_count_[0]\n",
    "\n",
    "tokens['neg'] = tokens['neg'] / nb.class_count_[1]\n",
    "\n",
    "tokens.sample(5, random_state=6)\n",
    "\n",
    "# calculate the ratio of neg-to-pos for each token\n",
    "\n",
    "tokens['neg_ratio'] = tokens['neg'] / tokens['pos']\n",
    "\n",
    "print(tokens.sample(5, random_state=6))\n",
    "\n",
    "# examine the DataFrame sorted by spam_ratio\n",
    "\n",
    "# note: use sort() instead of sort_values() for pandas 0.16.2 and earlier\n",
    "\n",
    "print(tokens.sort_values('neg_ratio', ascending=False))\n",
    "\n",
    "# look up the spam_ratio for a given token\n",
    "\n",
    "print(tokens.loc[\"good\", \"neg_ratio\"])\n",
    "\n",
    "['00', '000', '007', '00s', '03', '04', '05', '05425', '10', '100', '1000', '100m', '101', '102', '103', '104', '105', '106', '107', '108', '109', '10b', '10s', '10th', '11', '110', '111', '112', '113', '114', '115', '117', '118', '11th', '12', '123', '125', '126', '127', '1272', '128', '129', '1298', '12th', '13', '130', '1305', '131', '132', '133']\n",
    "['zi', 'zidler', 'ziegler', 'ziembicki', 'zigged', 'ziggy', 'zilch', 'zimbabwe', 'zimmely', 'zimmer', 'zimmerly', 'zinger', 'zingers', 'zinnia', 'zip', 'zipped', 'zippel', 'zipper', 'zippers', 'zippy', 'zips', 'ziyi', 'zodiac', 'zoe', 'zombie', 'zombies', 'zombified', 'zone', 'zones', 'zoo', 'zookeeper', 'zoolander', 'zoologist', 'zoom', 'zooming', 'zooms', 'zoot', 'zophres', 'zorg', 'zorro', 'zsigmond', 'zucker', 'zuehlke', 'zuko', 'zukovsky', 'zulu', 'zwick', 'zwigoff', 'zycie', 'zzzzzzz']\n",
    "[[  5.  34.   3. ...,   0.   0.   1.]\n",
    " [  1.  33.   6. ...,   1.   2.   0.]]\n",
    "(2, 34197)\n",
    "[  5.  34.   3. ...,   0.   0.   1.]\n",
    "[  1.  33.   6. ...,   1.   2.   0.]\n",
    "        neg   pos\n",
    "token            \n",
    "00      1.0   5.0\n",
    "000    33.0  34.0\n",
    "007     6.0   3.0\n",
    "00s     0.0   1.0\n",
    "03      0.0   2.0\n",
    "                 neg  pos\n",
    "token                    \n",
    "psyches          1.0  0.0\n",
    "finnegan         0.0  3.0\n",
    "trustworthiness  1.0  0.0\n",
    "patton           2.0  2.0\n",
    "salivate         0.0  1.0\n",
    "[ 665.  675.]\n",
    "                      neg       pos  neg_ratio\n",
    "token                                         \n",
    "psyches          0.002963  0.001504   1.970370\n",
    "finnegan         0.001481  0.006015   0.246296\n",
    "trustworthiness  0.002963  0.001504   1.970370\n",
    "patton           0.004444  0.004511   0.985185\n",
    "salivate         0.001481  0.003008   0.492593\n",
    "                    neg       pos  neg_ratio\n",
    "token                                       \n",
    "mulan          0.142222  0.001504  94.577778\n",
    "flynt          0.118519  0.001504  78.814815\n",
    "sweetback      0.045926  0.001504  30.540741\n",
    "ordell         0.044444  0.001504  29.555556\n",
    "hedwig         0.042963  0.001504  28.570370\n",
    "argento        0.041481  0.001504  27.585185\n",
    "taran          0.040000  0.001504  26.600000\n",
    "pleasantville  0.038519  0.001504  25.614815\n",
    "lambeau        0.038519  0.001504  25.614815\n",
    "fei            0.038519  0.001504  25.614815\n",
    "lebowski       0.075556  0.003008  25.122222\n",
    "chad           0.037037  0.001504  24.629630\n",
    "mallory        0.035556  0.001504  23.644444\n",
    "matilda        0.034074  0.001504  22.659259\n",
    "rounders       0.032593  0.001504  21.674074\n",
    "carver         0.032593  0.001504  21.674074\n",
    "lumumba        0.032593  0.001504  21.674074\n",
    "redford        0.031111  0.001504  20.688889\n",
    "rico           0.031111  0.001504  20.688889\n",
    "cauldron       0.062222  0.003008  20.688889\n",
    "shrek          0.029630  0.001504  19.703704\n",
    "maximus        0.028148  0.001504  18.718519\n",
    "dolores        0.028148  0.001504  18.718519\n",
    "bubby          0.028148  0.001504  18.718519\n",
    "capone         0.026667  0.001504  17.733333\n",
    "gale           0.026667  0.001504  17.733333\n",
    "hen            0.026667  0.001504  17.733333\n",
    "bianca         0.026667  0.001504  17.733333\n",
    "motta          0.026667  0.001504  17.733333\n",
    "damon          0.053333  0.003008  17.733333\n",
    "...                 ...       ...        ...\n",
    "silverman      0.001481  0.021053   0.070370\n",
    "zach           0.001481  0.021053   0.070370\n",
    "bont           0.001481  0.021053   0.070370\n",
    "vikings        0.001481  0.021053   0.070370\n",
    "silverstone    0.001481  0.021053   0.070370\n",
    "sphere         0.001481  0.021053   0.070370\n",
    "dwayne         0.001481  0.021053   0.070370\n",
    "grinch         0.001481  0.022556   0.065679\n",
    "caulder        0.001481  0.022556   0.065679\n",
    "musketeer      0.001481  0.022556   0.065679\n",
    "webb           0.002963  0.046617   0.063560\n",
    "jericho        0.001481  0.024060   0.061574\n",
    "brenner        0.001481  0.024060   0.061574\n",
    "macdonald      0.001481  0.025564   0.057952\n",
    "psychlo        0.001481  0.025564   0.057952\n",
    "bilko          0.001481  0.025564   0.057952\n",
    "sinise         0.001481  0.027068   0.054733\n",
    "eszterhas      0.001481  0.027068   0.054733\n",
    "mandingo       0.001481  0.027068   0.054733\n",
    "kersey         0.001481  0.027068   0.054733\n",
    "bronson        0.001481  0.027068   0.054733\n",
    "alicia         0.002963  0.057143   0.051852\n",
    "schumacher     0.002963  0.061654   0.048058\n",
    "hewitt         0.001481  0.034586   0.042834\n",
    "crawford       0.001481  0.036090   0.041049\n",
    "freddie        0.001481  0.040602   0.036488\n",
    "jakob          0.001481  0.042105   0.035185\n",
    "prinze         0.001481  0.042105   0.035185\n",
    "seagal         0.002963  0.094737   0.031276\n",
    "nbsp           0.001481  0.088722   0.016698\n",
    "\n",
    "[34197 rows x 3 columns]\n",
    "1.0544946957\n",
    "\n",
    "#same parameter to vectorize data in order to compare to precedent methods\n",
    "\n",
    "# Logistic regression\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range =(1,2), max_df =0.7, min_df=2)),\n",
    "\n",
    "                     ('nb', LogisticRegression()),\n",
    "\n",
    "                    ])\n",
    "\n",
    "​\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_class = text_clf.predict(X_test)\n",
    "\n",
    "​\n",
    "\n",
    "y_pred_prob = text_clf.predict(X_test)\n",
    "\n",
    "​\n",
    "\n",
    "metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "​\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_class,\n",
    "\n",
    "    target_names=[\"pos\",\"neg\"]))\n",
    "\n",
    "metrics.accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# With TF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "​\n",
    "\n",
    "vect = CountVectorizer(stop_words='english', ngram_range =(1,2), max_df =0.7, min_df=2)\n",
    "\n",
    "​\n",
    "\n",
    "# combine fit and transform into a single step\n",
    "\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "\n",
    "#look at the training data\n",
    "\n",
    "pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())\n",
    "\n",
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "X_test_dtm\n",
    "\n",
    "​\n",
    "\n",
    "#Compute TF\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False)\n",
    "\n",
    "X_train_tf = tf_transformer.fit_transform(X_train_dtm)\n",
    "\n",
    "X_train_tf.shape\n",
    "\n",
    "X_test_tf = TfidTransformer(use_idf=False).fit_transform(X_train_dtm)\n",
    "\n",
    "#Train logistic regression with TF representation\n",
    "\n",
    "lr = LogisticRegression().fit(X_train_tf, y_train)\n",
    "\n",
    "​\n",
    "\n",
    "y_pred_class = lr.predict(X_test_tf)\n",
    "\n",
    "​\n",
    "\n",
    "y_pred_prob = lr.predict_proba(X_test_tf)\n",
    "\n",
    "​\n",
    "\n",
    "metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "​\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_class,\n",
    "\n",
    "    target_names=[\"pos\",\"neg\"]))\n",
    "\n",
    "metrics.accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "<ipython-input-31-5aced182d9df> in <module>\n",
    "     16 X_train_tf = tf_transformer.fit_transform(X_train_dtm)\n",
    "     17 X_train_tf.shape\n",
    "---> 18 X_test_tf = TfidTransformer(use_idf=False).fit_transform(X_train_dtm)\n",
    "     19 #Train logistic regression with TF representation\n",
    "     20 lr = LogisticRegression().fit(X_train_tf, y_train)\n",
    "\n",
    "NameError: name 'TfidTransformer' is not defined\n",
    "\n",
    "​\n",
    "\n",
    "### Computing TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_dtm)\n",
    "\n",
    "X_train_tfidf.shape\n",
    "\n",
    "​\n",
    "\n",
    "nb_tfidf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "​\n",
    "\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_dtm)\n",
    "\n",
    "print(X_test_tfidf.shape)\n",
    "\n",
    "y_pred_class_tfidf = nb_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "​\n",
    "\n",
    "metrics.accuracy_score(y_test, y_pred_class_tfidf)\n",
    "\n",
    "​\n",
    "\n",
    "plot_confusion_matrix(metrics.confusion_matrix(y_test, y_pred_class_tfidf),[])\n",
    "\n",
    "(660, 50456)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-46-7c07a568b88d> in <module>\n",
    "     14 metrics.accuracy_score(y_test, y_pred_class_tfidf)\n",
    "     15 \n",
    "---> 16 plot_confusion_matrix(metrics.confusion_matrix(y_test, y_pred_class_tfidf),[])\n",
    "\n",
    "/usr/local/lib/python3.5/dist-packages/scikitplot/metrics.py in plot_confusion_matrix(y_true, y_pred, labels, true_labels, pred_labels, title, normalize, hide_zeros, hide_counts, x_tick_rotation, ax, figsize, cmap, title_fontsize, text_fontsize)\n",
    "    115         fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    116 \n",
    "--> 117     cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    118     if labels is None:\n",
    "    119         classes = unique_labels(y_true, y_pred)\n",
    "\n",
    "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py in confusion_matrix(y_true, y_pred, labels, sample_weight)\n",
    "    251 \n",
    "    252     \"\"\"\n",
    "--> 253     y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
    "    254     if y_type not in (\"binary\", \"multiclass\"):\n",
    "    255         raise ValueError(\"%s is not supported\" % y_type)\n",
    "\n",
    "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py in _check_targets(y_true, y_pred)\n",
    "     69     y_pred : array or indicator matrix\n",
    "     70     \"\"\"\n",
    "---> 71     check_consistent_length(y_true, y_pred)\n",
    "     72     type_true = type_of_target(y_true)\n",
    "     73     type_pred = type_of_target(y_pred)\n",
    "\n",
    "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py in check_consistent_length(*arrays)\n",
    "    228     if len(uniques) > 1:\n",
    "    229         raise ValueError(\"Found input variables with inconsistent numbers of\"\n",
    "--> 230                          \" samples: %r\" % [int(l) for l in lengths])\n",
    "    231 \n",
    "    232 \n",
    "\n",
    "ValueError: Found input variables with inconsistent numbers of samples: [2, 0]\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "AttributeError                            Traceback (most recent call last)\n",
    "/usr/local/lib/python3.5/dist-packages/ipykernel/pylab/backend_inline.py in show(close, block)\n",
    "     37             display(\n",
    "     38                 figure_manager.canvas.figure,\n",
    "---> 39                 metadata=_fetch_figure_metadata(figure_manager.canvas.figure)\n",
    "     40             )\n",
    "     41     finally:\n",
    "\n",
    "/usr/local/lib/python3.5/dist-packages/ipykernel/pylab/backend_inline.py in _fetch_figure_metadata(fig)\n",
    "    172     \"\"\"Get some metadata to help with displaying a figure.\"\"\"\n",
    "    173     # determine if a background is needed for legibility\n",
    "--> 174     if _is_transparent(fig.get_facecolor()):\n",
    "    175         # the background is transparent\n",
    "    176         ticksLight = _is_light([label.get_color()\n",
    "\n",
    "/usr/local/lib/python3.5/dist-packages/ipykernel/pylab/backend_inline.py in _is_transparent(color)\n",
    "    193 def _is_transparent(color):\n",
    "    194     \"\"\"Determine transparency from alpha.\"\"\"\n",
    "--> 195     rgba = colors.to_rgba(color)\n",
    "    196     return rgba[3] < .5\n",
    "\n",
    "AttributeError: module 'matplotlib.colors' has no attribute 'to_rgba'\n",
    "\n",
    "#USE STRING KERNELS : not done until now in others group and suggested at the very end of the teacher's notebook\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#test a range of hyperparameters\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', max_df =0.7, min_df=2)),\n",
    "\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "\n",
    "                     ('knn', KNeighborsClassifier()),\n",
    "\n",
    "                    ])\n",
    "\n",
    "​\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "\n",
    "              'tfidf__use_idf' : (True,False),\n",
    "\n",
    "              'knn__n_neighbors': (10, 25, 50),\n",
    "\n",
    "              'knn__p' : (1,2),\n",
    "\n",
    "              'knn__weights': ('uniform', 'distance'),\n",
    "\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "​\n",
    "\n",
    "gs_clf.fit(X_train, y_train)\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
    "  warnings.warn(CV_WARNING, FutureWarning)\n",
    "\n",
    "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
    "       estimator=Pipeline(memory=None,\n",
    "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
    "        lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
    "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
    "        ...ki',\n",
    "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
    "           weights='uniform'))]),\n",
    "       fit_params=None, iid='warn', n_jobs=-1,\n",
    "       param_grid={'knn__n_neighbors': (10, 25, 50), 'tfidf__use_idf': (True, False), 'vect__ngram_range': [(1, 1), (1, 2)], 'knn__weights': ('uniform', 'distance'), 'knn__p': (1, 2)},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
    "       scoring=None, verbose=0)\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "\n",
    "print(gs_clf.best_score_)\n",
    "\n",
    "df = pd.DataFrame(gs_clf.cv_results_)\n",
    "\n",
    "print(df.sort_values('rank',ascending=True))\n",
    "\n",
    "knn__n_neighbors: 50\n",
    "knn__p: 2\n",
    "knn__weights: 'distance'\n",
    "tfidf__use_idf: True\n",
    "vect__ngram_range: (1, 1)\n",
    "0.764179104478\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "<ipython-input-26-cf35602fc9d0> in <module>\n",
    "      3 print(gs_clf.best_score_)\n",
    "      4 df = pd.DataFrame(gs_clf.cv_results_)\n",
    "----> 5 print(df.sort_values('rank',ascending=True))\n",
    "\n",
    "/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py in sort_values(self, by, axis, ascending, inplace, kind, na_position)\n",
    "   4419             by = by[0]\n",
    "   4420             k = self._get_label_or_level_values(by, axis=axis,\n",
    "-> 4421                                                 stacklevel=stacklevel)\n",
    "   4422 \n",
    "   4423             if isinstance(ascending, (tuple, list)):\n",
    "\n",
    "/usr/local/lib/python3.5/dist-packages/pandas/core/generic.py in _get_label_or_level_values(self, key, axis, stacklevel)\n",
    "   1380             values = self.axes[axis].get_level_values(key)._values\n",
    "   1381         else:\n",
    "-> 1382             raise KeyError(key)\n",
    "   1383 \n",
    "   1384         # Check for duplicates\n",
    "\n",
    "KeyError: 'rank'\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#test a range of hyperparameters\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', max_df =0.7, min_df=2)),\n",
    "\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "\n",
    "                     ('knn', RandomForestClassifier()),\n",
    "\n",
    "                    ])\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "\n",
    "              'tfidf__use_idf' : (True,False),\n",
    "\n",
    "              'n_estimators': (10, 25, 50),\n",
    "\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "print(gs_clf.best_score_)\n",
    "df = pd.DataFrame(gs_clf.cv_results_)\n",
    "print(df.sort_values('rank',ascending=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
