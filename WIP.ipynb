{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from scikitplot.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>neg</td>\n",
       "      <td>'traffic violation' dr . daniel's review of u-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>neg</td>\n",
       "      <td>this movie is written by the man who is deemed...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>pos</td>\n",
       "      <td>the disney studios has its formula for annual ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>pos</td>\n",
       "      <td>at first glance , it appears that the home alo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>pos</td>\n",
       "      <td>\" a bug's life \" may not be \" toy story , \" b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text  label_num\n",
       "1322   neg  'traffic violation' dr . daniel's review of u-...          0\n",
       "1508   neg  this movie is written by the man who is deemed...          0\n",
       "945    pos  the disney studios has its formula for annual ...          1\n",
       "698    pos  at first glance , it appears that the home alo...          1\n",
       "440    pos   \" a bug's life \" may not be \" toy story , \" b...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get all files path\n",
    "posFiles = glob('review_polarity/txt_sentoken/pos/*')\n",
    "negFiles = glob('review_polarity/txt_sentoken/neg/*')\n",
    "#read text files\n",
    "posReviews = np.array([open(f).read() for f in posFiles])\n",
    "negReviews = np.array([open(f).read() for f in negFiles])\n",
    "#use pandas to label and mix the data\n",
    "polarity_files_df = pd.DataFrame({'pos':posReviews,'neg':negReviews})\n",
    "polarity_files_df = pd.melt(polarity_files_df, value_vars=['pos','neg'],value_name=\"text\",var_name=\"label\")\n",
    "polarity_files_df[\"label_num\"] = polarity_files_df.label.map({\"neg\":0, \"pos\":1})\n",
    "polarity_files_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340,)\n",
      "(660,)\n",
      "(1340,)\n",
      "(660,)\n"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(polarity_files_df.text, polarity_files_df.label_num, test_size=0.33, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos       0.80      0.78      0.79       335\n",
      "         neg       0.78      0.80      0.79       325\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       660\n",
      "   macro avg       0.79      0.79      0.79       660\n",
      "weighted avg       0.79      0.79      0.79       660\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78939393939393943"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove English stop words\n",
    "# include 1-grams and 2-grams (if 3-grams : seems not better because of the length of the vector)\n",
    "# ignore terms that appear in more than 70% of the documents (intuitively meaningful, it is indeed the best multiple of 10% to have a good score )\n",
    "# only keep terms that appear in at least 2 documents\n",
    "\n",
    "# TODO : remove 'not' etc. from stopwords to not remove 'not' from sentences like 'this film is not bad'\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range =(1,2), max_df =0.7, min_df=2)),\n",
    "                     ('nb', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "# à vérifier : pos et neg sont biens les pos et neg\n",
    "print(metrics.classification_report(y_test, y_pred,\n",
    "    target_names=[\"pos\",\"neg\"]))\n",
    "metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "#plt.figure()\n",
    "#plot_confusion_matrix(cnf_matrix, classes=['pos','neg'],\n",
    "#                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "#plt.figure()\n",
    "#plot_confusion_matrix(cnf_matrix, classes=['pos','neg'], normalize=True,\n",
    "#                      title='Normalized confusion matrix')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'nb': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " 'nb__alpha': 1.0,\n",
       " 'nb__class_prior': None,\n",
       " 'nb__fit_prior': True,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
       "           ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
       "         ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 0.7,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 2,\n",
       " 'vect__ngram_range': (1, 3),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': 'english',\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': None,\n",
       " 'vect__vocabulary': None}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test a range of hyperparameters\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range =(1,3), max_df =0.7, min_df=2)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('nb', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)  \n",
    "text_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#TO ADAPT depending on what text_clf.get_params() displays\n",
    "#how did we look for a good parameter alpha : try with 1e-3 and 1e-2 : 1e-2 is the best so we tried with 1e0 and 1e-1 and 1e-2 : 1e-1 is the best one\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'nb__alpha': (1e-2, 1e-1),\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82835820895522383"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "y_pred = gs_clf.predict(X_test)\n",
    "\n",
    "gs_clf.best_score_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb__alpha: 0.1\n",
      "tfidf__use_idf: False\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        ...False,\n",
       "         use_idf=False)), ('nb', MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_estimator_.get_params()[\"nb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.90705363,  2.97942503,  0.81350295,  3.52606408,  0.84179568,\n",
       "         3.05628093,  0.86787113,  2.99055672,  0.8450772 ,  2.92341105,\n",
       "         0.79071728,  2.67280912]),\n",
       " 'mean_score_time': array([ 0.36827596,  0.56174795,  0.35869686,  0.54121161,  0.4139332 ,\n",
       "         0.64748732,  0.40891202,  0.57405368,  0.3593332 ,  0.57712452,\n",
       "         0.34732358,  0.47334862]),\n",
       " 'mean_test_score': array([ 0.80074627,  0.79328358,  0.82835821,  0.82014925,  0.77835821,\n",
       "         0.77537313,  0.80149254,  0.79626866,  0.76119403,  0.76343284,\n",
       "         0.78283582,  0.77835821]),\n",
       " 'mean_train_score': array([ 0.99291072,  0.9981353 ,  0.98656841,  0.99515037,  0.99776203,\n",
       "         1.        ,  0.99253787,  0.99925429,  0.99925429,  1.        ,\n",
       "         0.99776203,  1.        ]),\n",
       " 'param_nb__alpha': masked_array(data = [0.1 0.1 0.1 0.1 0.01 0.01 0.01 0.01 0.001 0.001 0.001 0.001],\n",
       "              mask = [False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_tfidf__use_idf': masked_array(data = [True True False False True True False False True True False False],\n",
       "              mask = [False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__ngram_range': masked_array(data = [(1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2)\n",
       "  (1, 1) (1, 2)],\n",
       "              mask = [False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'nb__alpha': 0.1,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'nb__alpha': 0.1, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
       "  {'nb__alpha': 0.1, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
       "  {'nb__alpha': 0.1, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)},\n",
       "  {'nb__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)},\n",
       "  {'nb__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
       "  {'nb__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
       "  {'nb__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)},\n",
       "  {'nb__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)},\n",
       "  {'nb__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
       "  {'nb__alpha': 0.001, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
       "  {'nb__alpha': 0.001, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}],\n",
       " 'rank_test_score': array([ 4,  6,  1,  2,  8, 10,  3,  5, 12, 11,  7,  8], dtype=int32),\n",
       " 'split0_test_score': array([ 0.81431767,  0.80760626,  0.8344519 ,  0.82997763,  0.80089485,\n",
       "         0.80089485,  0.81655481,  0.81879195,  0.7852349 ,  0.78747204,\n",
       "         0.80313199,  0.80089485]),\n",
       " 'split0_train_score': array([ 0.99216125,  1.        ,  0.98656215,  0.99552072,  0.99888018,\n",
       "         1.        ,  0.99216125,  1.        ,  1.        ,  1.        ,\n",
       "         0.99888018,  1.        ]),\n",
       " 'split1_test_score': array([ 0.81655481,  0.80536913,  0.85011186,  0.83221477,  0.79418345,\n",
       "         0.77628635,  0.82102908,  0.8098434 ,  0.76510067,  0.76286353,\n",
       "         0.79642058,  0.77852349]),\n",
       " 'split1_train_score': array([ 0.9944009 ,  0.99888018,  0.98992161,  0.99776036,  0.99888018,\n",
       "         1.        ,  0.9944009 ,  1.        ,  1.        ,  1.        ,\n",
       "         0.99888018,  1.        ]),\n",
       " 'split2_test_score': array([ 0.77130045,  0.76681614,  0.80044843,  0.79820628,  0.73991031,\n",
       "         0.74887892,  0.76681614,  0.76008969,  0.73318386,  0.73991031,\n",
       "         0.74887892,  0.75560538]),\n",
       " 'split2_train_score': array([ 0.99217002,  0.99552573,  0.98322148,  0.99217002,  0.99552573,\n",
       "         1.        ,  0.99105145,  0.99776286,  0.99776286,  1.        ,\n",
       "         0.99552573,  1.        ]),\n",
       " 'std_fit_time': array([ 0.05344825,  0.22817698,  0.08103768,  0.14229889,  0.06774117,\n",
       "         0.15168821,  0.05769988,  0.10017736,  0.0529942 ,  0.07123096,\n",
       "         0.01431095,  0.09777595]),\n",
       " 'std_score_time': array([ 0.01425502,  0.01814319,  0.0188923 ,  0.02524657,  0.05019106,\n",
       "         0.05670544,  0.05020673,  0.05297161,  0.0030775 ,  0.04299396,\n",
       "         0.01706694,  0.08716292]),\n",
       " 'std_test_score': array([ 0.02081809,  0.01871667,  0.02072461,  0.01552557,  0.02729431,\n",
       "         0.02124127,  0.02456054,  0.02581377,  0.0214248 ,  0.01941753,\n",
       "         0.02414039,  0.01848626]),\n",
       " 'std_train_score': array([ 0.00105372,  0.00190104,  0.00273532,  0.00229722,  0.0015813 ,\n",
       "         0.        ,  0.00139309,  0.0010546 ,  0.0010546 ,  0.        ,\n",
       "         0.0015813 ,  0.        ])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0        0.907054         0.368276         0.800746          0.992911   \n",
      "1        2.979425         0.561748         0.793284          0.998135   \n",
      "2        0.813503         0.358697         0.828358          0.986568   \n",
      "3        3.526064         0.541212         0.820149          0.995150   \n",
      "4        0.841796         0.413933         0.778358          0.997762   \n",
      "5        3.056281         0.647487         0.775373          1.000000   \n",
      "6        0.867871         0.408912         0.801493          0.992538   \n",
      "7        2.990557         0.574054         0.796269          0.999254   \n",
      "8        0.845077         0.359333         0.761194          0.999254   \n",
      "9        2.923411         0.577125         0.763433          1.000000   \n",
      "10       0.790717         0.347324         0.782836          0.997762   \n",
      "11       2.672809         0.473349         0.778358          1.000000   \n",
      "\n",
      "   param_nb__alpha param_tfidf__use_idf param_vect__ngram_range  \\\n",
      "0              0.1                 True                  (1, 1)   \n",
      "1              0.1                 True                  (1, 2)   \n",
      "2              0.1                False                  (1, 1)   \n",
      "3              0.1                False                  (1, 2)   \n",
      "4             0.01                 True                  (1, 1)   \n",
      "5             0.01                 True                  (1, 2)   \n",
      "6             0.01                False                  (1, 1)   \n",
      "7             0.01                False                  (1, 2)   \n",
      "8            0.001                 True                  (1, 1)   \n",
      "9            0.001                 True                  (1, 2)   \n",
      "10           0.001                False                  (1, 1)   \n",
      "11           0.001                False                  (1, 2)   \n",
      "\n",
      "                                               params  rank_test_score  \\\n",
      "0   {'nb__alpha': 0.1, 'vect__ngram_range': (1, 1)...                4   \n",
      "1   {'nb__alpha': 0.1, 'vect__ngram_range': (1, 2)...                6   \n",
      "2   {'nb__alpha': 0.1, 'vect__ngram_range': (1, 1)...                1   \n",
      "3   {'nb__alpha': 0.1, 'vect__ngram_range': (1, 2)...                2   \n",
      "4   {'nb__alpha': 0.01, 'vect__ngram_range': (1, 1...                8   \n",
      "5   {'nb__alpha': 0.01, 'vect__ngram_range': (1, 2...               10   \n",
      "6   {'nb__alpha': 0.01, 'vect__ngram_range': (1, 1...                3   \n",
      "7   {'nb__alpha': 0.01, 'vect__ngram_range': (1, 2...                5   \n",
      "8   {'nb__alpha': 0.001, 'vect__ngram_range': (1, ...               12   \n",
      "9   {'nb__alpha': 0.001, 'vect__ngram_range': (1, ...               11   \n",
      "10  {'nb__alpha': 0.001, 'vect__ngram_range': (1, ...                7   \n",
      "11  {'nb__alpha': 0.001, 'vect__ngram_range': (1, ...                8   \n",
      "\n",
      "    split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0            0.814318            0.992161           0.816555   \n",
      "1            0.807606            1.000000           0.805369   \n",
      "2            0.834452            0.986562           0.850112   \n",
      "3            0.829978            0.995521           0.832215   \n",
      "4            0.800895            0.998880           0.794183   \n",
      "5            0.800895            1.000000           0.776286   \n",
      "6            0.816555            0.992161           0.821029   \n",
      "7            0.818792            1.000000           0.809843   \n",
      "8            0.785235            1.000000           0.765101   \n",
      "9            0.787472            1.000000           0.762864   \n",
      "10           0.803132            0.998880           0.796421   \n",
      "11           0.800895            1.000000           0.778523   \n",
      "\n",
      "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0             0.994401           0.771300            0.992170      0.053448   \n",
      "1             0.998880           0.766816            0.995526      0.228177   \n",
      "2             0.989922           0.800448            0.983221      0.081038   \n",
      "3             0.997760           0.798206            0.992170      0.142299   \n",
      "4             0.998880           0.739910            0.995526      0.067741   \n",
      "5             1.000000           0.748879            1.000000      0.151688   \n",
      "6             0.994401           0.766816            0.991051      0.057700   \n",
      "7             1.000000           0.760090            0.997763      0.100177   \n",
      "8             1.000000           0.733184            0.997763      0.052994   \n",
      "9             1.000000           0.739910            1.000000      0.071231   \n",
      "10            0.998880           0.748879            0.995526      0.014311   \n",
      "11            1.000000           0.755605            1.000000      0.097776   \n",
      "\n",
      "    std_score_time  std_test_score  std_train_score  \n",
      "0         0.014255        0.020818         0.001054  \n",
      "1         0.018143        0.018717         0.001901  \n",
      "2         0.018892        0.020725         0.002735  \n",
      "3         0.025247        0.015526         0.002297  \n",
      "4         0.050191        0.027294         0.001581  \n",
      "5         0.056705        0.021241         0.000000  \n",
      "6         0.050207        0.024561         0.001393  \n",
      "7         0.052972        0.025814         0.001055  \n",
      "8         0.003077        0.021425         0.001055  \n",
      "9         0.042994        0.019418         0.000000  \n",
      "10        0.017067        0.024140         0.001581  \n",
      "11        0.087163        0.018486         0.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(gs_clf.cv_results_)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '007', '00s', '03', '04', '05', '05425', '10', '100', '1000', '100m', '101', '102', '103', '104', '105', '106', '107', '108', '109', '10b', '10s', '10th', '11', '110', '111', '112', '113', '114', '115', '117', '118', '11th', '12', '123', '125', '126', '127', '1272', '128', '129', '1298', '12th', '13', '130', '1305', '131', '132', '133']\n",
      "['zi', 'zidler', 'ziegler', 'ziembicki', 'zigged', 'ziggy', 'zilch', 'zimbabwe', 'zimmely', 'zimmer', 'zimmerly', 'zinger', 'zingers', 'zinnia', 'zip', 'zipped', 'zippel', 'zipper', 'zippers', 'zippy', 'zips', 'ziyi', 'zodiac', 'zoe', 'zombie', 'zombies', 'zombified', 'zone', 'zones', 'zoo', 'zookeeper', 'zoolander', 'zoologist', 'zoom', 'zooming', 'zooms', 'zoot', 'zophres', 'zorg', 'zorro', 'zsigmond', 'zucker', 'zuehlke', 'zuko', 'zukovsky', 'zulu', 'zwick', 'zwigoff', 'zycie', 'zzzzzzz']\n",
      "[[  5.  34.   3. ...,   0.   0.   1.]\n",
      " [  1.  33.   6. ...,   1.   2.   0.]]\n",
      "(2, 34197)\n",
      "[  5.  34.   3. ...,   0.   0.   1.]\n",
      "[  1.  33.   6. ...,   1.   2.   0.]\n",
      "        neg   pos\n",
      "token            \n",
      "00      1.0   5.0\n",
      "000    33.0  34.0\n",
      "007     6.0   3.0\n",
      "00s     0.0   1.0\n",
      "03      0.0   2.0\n",
      "                 neg  pos\n",
      "token                    \n",
      "psyches          1.0  0.0\n",
      "finnegan         0.0  3.0\n",
      "trustworthiness  1.0  0.0\n",
      "patton           2.0  2.0\n",
      "salivate         0.0  1.0\n",
      "[ 665.  675.]\n",
      "                      neg       pos  neg_ratio\n",
      "token                                         \n",
      "psyches          0.002963  0.001504   1.970370\n",
      "finnegan         0.001481  0.006015   0.246296\n",
      "trustworthiness  0.002963  0.001504   1.970370\n",
      "patton           0.004444  0.004511   0.985185\n",
      "salivate         0.001481  0.003008   0.492593\n",
      "                    neg       pos  neg_ratio\n",
      "token                                       \n",
      "mulan          0.142222  0.001504  94.577778\n",
      "flynt          0.118519  0.001504  78.814815\n",
      "sweetback      0.045926  0.001504  30.540741\n",
      "ordell         0.044444  0.001504  29.555556\n",
      "hedwig         0.042963  0.001504  28.570370\n",
      "argento        0.041481  0.001504  27.585185\n",
      "taran          0.040000  0.001504  26.600000\n",
      "pleasantville  0.038519  0.001504  25.614815\n",
      "lambeau        0.038519  0.001504  25.614815\n",
      "fei            0.038519  0.001504  25.614815\n",
      "lebowski       0.075556  0.003008  25.122222\n",
      "chad           0.037037  0.001504  24.629630\n",
      "mallory        0.035556  0.001504  23.644444\n",
      "matilda        0.034074  0.001504  22.659259\n",
      "rounders       0.032593  0.001504  21.674074\n",
      "carver         0.032593  0.001504  21.674074\n",
      "lumumba        0.032593  0.001504  21.674074\n",
      "redford        0.031111  0.001504  20.688889\n",
      "rico           0.031111  0.001504  20.688889\n",
      "cauldron       0.062222  0.003008  20.688889\n",
      "shrek          0.029630  0.001504  19.703704\n",
      "maximus        0.028148  0.001504  18.718519\n",
      "dolores        0.028148  0.001504  18.718519\n",
      "bubby          0.028148  0.001504  18.718519\n",
      "capone         0.026667  0.001504  17.733333\n",
      "gale           0.026667  0.001504  17.733333\n",
      "hen            0.026667  0.001504  17.733333\n",
      "bianca         0.026667  0.001504  17.733333\n",
      "motta          0.026667  0.001504  17.733333\n",
      "damon          0.053333  0.003008  17.733333\n",
      "...                 ...       ...        ...\n",
      "silverman      0.001481  0.021053   0.070370\n",
      "zach           0.001481  0.021053   0.070370\n",
      "bont           0.001481  0.021053   0.070370\n",
      "vikings        0.001481  0.021053   0.070370\n",
      "silverstone    0.001481  0.021053   0.070370\n",
      "sphere         0.001481  0.021053   0.070370\n",
      "dwayne         0.001481  0.021053   0.070370\n",
      "grinch         0.001481  0.022556   0.065679\n",
      "caulder        0.001481  0.022556   0.065679\n",
      "musketeer      0.001481  0.022556   0.065679\n",
      "webb           0.002963  0.046617   0.063560\n",
      "jericho        0.001481  0.024060   0.061574\n",
      "brenner        0.001481  0.024060   0.061574\n",
      "macdonald      0.001481  0.025564   0.057952\n",
      "psychlo        0.001481  0.025564   0.057952\n",
      "bilko          0.001481  0.025564   0.057952\n",
      "sinise         0.001481  0.027068   0.054733\n",
      "eszterhas      0.001481  0.027068   0.054733\n",
      "mandingo       0.001481  0.027068   0.054733\n",
      "kersey         0.001481  0.027068   0.054733\n",
      "bronson        0.001481  0.027068   0.054733\n",
      "alicia         0.002963  0.057143   0.051852\n",
      "schumacher     0.002963  0.061654   0.048058\n",
      "hewitt         0.001481  0.034586   0.042834\n",
      "crawford       0.001481  0.036090   0.041049\n",
      "freddie        0.001481  0.040602   0.036488\n",
      "jakob          0.001481  0.042105   0.035185\n",
      "prinze         0.001481  0.042105   0.035185\n",
      "seagal         0.002963  0.094737   0.031276\n",
      "nbsp           0.001481  0.088722   0.016698\n",
      "\n",
      "[34197 rows x 3 columns]\n",
      "1.0544946957\n"
     ]
    }
   ],
   "source": [
    "##TODO : neg are neg ? pos are pos ?\n",
    "nb = MultinomialNB()\n",
    "# store the vocabulary of X_train\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "X_train_tokens = vect.get_feature_names()\n",
    "len(X_train_tokens)\n",
    "# examine the first 50 tokens\n",
    "print(X_train_tokens[0:50])\n",
    "# examine the last 50 tokens\n",
    "print(X_train_tokens[-50:])\n",
    "# Naive Bayes counts the number of times each token appears in each class\n",
    "# trailing underscore is scikit convention for attributes that are learned during model fitting\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "nb = nb.fit(X_train_dtm, y_train)\n",
    "print(nb.feature_count_)\n",
    "# rows represent classes, columns represent tokens\n",
    "print(nb.feature_count_.shape)\n",
    "# number of times each token appears across all HAM messages\n",
    "pos_token_count = nb.feature_count_[0, :]\n",
    "print(pos_token_count)\n",
    "# number of times each token appears across all SPAM messages\n",
    "neg_token_count = nb.feature_count_[1, :]\n",
    "print(neg_token_count)\n",
    "# create a DataFrame of tokens with their separate ham and spam counts\n",
    "tokens = pd.DataFrame({\"token\":X_train_tokens, \"pos\":pos_token_count, \"neg\":neg_token_count}).set_index(\"token\")\n",
    "print(tokens.head())\n",
    "# examine 5 random DataFrame rows\n",
    "print(tokens.sample(5, random_state=6))\n",
    "# Naive Bayes counts the number of observations in each class\n",
    "print(nb.class_count_)\n",
    "# add 1 to ham and spam counts to avoid 0 probabilities\n",
    "tokens['pos'] = tokens['pos'] + 1\n",
    "tokens['neg'] = tokens['neg'] + 1\n",
    "tokens.sample(5, random_state=6)\n",
    "# convert the ham and spam counts into frequencies\n",
    "tokens['pos'] = tokens['pos'] / nb.class_count_[0]\n",
    "tokens['neg'] = tokens['neg'] / nb.class_count_[1]\n",
    "tokens.sample(5, random_state=6)\n",
    "# calculate the ratio of neg-to-pos for each token\n",
    "tokens['neg_ratio'] = tokens['neg'] / tokens['pos']\n",
    "print(tokens.sample(5, random_state=6))\n",
    "# examine the DataFrame sorted by spam_ratio\n",
    "# note: use sort() instead of sort_values() for pandas 0.16.2 and earlier\n",
    "print(tokens.sort_values('neg_ratio', ascending=False))\n",
    "# look up the spam_ratio for a given token\n",
    "print(tokens.loc[\"good\", \"neg_ratio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same parameter to vectorize data in order to compare to precedent methods\n",
    "# Logistic regression\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range =(1,2), max_df =0.7, min_df=2)),\n",
    "                     ('nb', LogisticRegression()),\n",
    "                    ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "y_pred_class = text_clf.predict(X_test)\n",
    "\n",
    "y_pred_prob = text_clf.predict(X_test)\n",
    "\n",
    "metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_class,\n",
    "    target_names=[\"pos\",\"neg\"]))\n",
    "metrics.accuracy_score(y_test, y_pred_class)\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5aced182d9df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mX_train_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_dtm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mX_train_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mX_test_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_dtm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#Train logistic regression with TF representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "# With TF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vect = CountVectorizer(stop_words='english', ngram_range =(1,2), max_df =0.7, min_df=2)\n",
    "\n",
    "# combine fit and transform into a single step\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "#look at the training data\n",
    "pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())\n",
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm\n",
    "\n",
    "#Compute TF\n",
    "tf_transformer = TfidfTransformer(use_idf=False)\n",
    "X_train_tf = tf_transformer.fit_transform(X_train_dtm)\n",
    "X_train_tf.shape\n",
    "X_test_tf = TfidTransformer(use_idf=False).fit_transform(X_train_dtm)\n",
    "#Train logistic regression with TF representation\n",
    "lr = LogisticRegression().fit(X_train_tf, y_train)\n",
    "\n",
    "y_pred_class = lr.predict(X_test_tf)\n",
    "\n",
    "y_pred_prob = lr.predict_proba(X_test_tf)\n",
    "\n",
    "metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_class,\n",
    "    target_names=[\"pos\",\"neg\"]))\n",
    "metrics.accuracy_score(y_test, y_pred_class)\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(660, 50456)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-7c07a568b88d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_class_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_class_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scikitplot/metrics.py\u001b[0m in \u001b[0;36mplot_confusion_matrix\u001b[0;34m(y_true, y_pred, labels, true_labels, pred_labels, title, normalize, hide_zeros, hide_counts, x_tick_rotation, ax, figsize, cmap, title_fontsize, text_fontsize)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 230\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2, 0]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.colors' has no attribute 'to_rgba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     37\u001b[0m             display(\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             )\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36m_fetch_figure_metadata\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;34m\"\"\"Get some metadata to help with displaying a figure.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;31m# determine if a background is needed for legibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_facecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;31m# the background is transparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         ticksLight = _is_light([label.get_color()\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36m_is_transparent\u001b[0;34m(color)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;34m\"\"\"Determine transparency from alpha.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrgba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.colors' has no attribute 'to_rgba'"
     ]
    }
   ],
   "source": [
    "\n",
    "### Computing TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_dtm)\n",
    "X_train_tfidf.shape\n",
    "\n",
    "nb_tfidf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_dtm)\n",
    "print(X_test_tfidf.shape)\n",
    "y_pred_class_tfidf = nb_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "metrics.accuracy_score(y_test, y_pred_class_tfidf)\n",
    "\n",
    "plot_confusion_matrix(metrics.confusion_matrix(y_test, y_pred_class_tfidf),[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE STRING KERNELS : not done until now in others group and suggested at the very end of the teacher's notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
